{"status":"ok","feed":{"url":"https://medium.com/feed/@renjietan","title":"Stories by Ren Jie Tan on Medium","link":"https://medium.com/@renjietan?source=rss-fdf264797c2a------2","author":"","description":"Stories by Ren Jie Tan on Medium","image":"https://cdn-images-1.medium.com/fit/c/150/150/1*nS3utlxM04JYyNy_x-JFKQ.jpeg"},"items":[{"title":"Breaking down Mean Average Precision (mAP)","pubDate":"2019-03-24 15:00:57","link":"https://towardsdatascience.com/breaking-down-mean-average-precision-map-ae462f623a52?source=rss-fdf264797c2a------2","guid":"https://medium.com/p/ae462f623a52","author":"Ren Jie Tan","thumbnail":"https://cdn-images-1.medium.com/max/1024/1*OOD94B3vflKRs8yIBwWgQQ.jpeg","description":"\n<h3>Breaking Down Mean Average Precision (mAP)</h3>\n<h4>Another metric for your data science\u00a0toolkit</h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*OOD94B3vflKRs8yIBwWgQQ.jpeg\"><figcaption>Photo by <a href=\"https://unsplash.com/@byadonia?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">\u015eahin Ye\u015filyaprak</a> on\u00a0<a href=\"https://medium.com/s/photos/mean-average-precision?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Unsplash</a></figcaption></figure><p>If you have come across the <a href=\"http://host.robots.ox.ac.uk/pascal/VOC/index.html\">PASCAL Visual Object Classes (VOC)</a> and <a href=\"http://cocodataset.org/#home\">MS Common Objects in Context (COCO)</a> challenge, or dabbled with projects involving information retrieval and re-identification (ReID), you might then be quite familiar with a metric called\u00a0mAP.</p>\n<p>The mean average precision (mAP) or sometimes simply just referred to as AP is a popular metric used to measure the performance of models doing document/information retrival and object detection tasks.</p>\n<p>The mean average precision (mAP) of a set of queries is defined by <a href=\"https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Mean_average_precision\">Wikipedia</a> as\u00a0such:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/245/1*0uDpXRwQU90HAQA1LkQMRQ.png\"><figcaption>Mean average precision formula given provided by Wikipedia</figcaption></figure><p>where <em>Q </em>is the number of queries in the set and AveP(q) is the average precision (AP) for a given query,\u00a0q.</p>\n<p>What the formula is essentially telling us is that, for a given query, q, we calculate its corresponding AP, and then the mean of the all these AP scores would give us a single number, called the mAP, which quantifies how good our model is at performing the\u00a0query.</p>\n<p>This definition baffles people (like me) who are just starting out in this field. I had questions like what is this set of queries? And what does AP mean? Is it just an average of the precision?</p>\n<p>This article hopes to address these questions and calculate mAP for both object detection and information retrieval tasks. This article would also be exploring why mAP is an appropriate and commonly used metric for information retrieval and object detection tasks.</p>\n<h3>Outline</h3>\n<ol>\n<li><a href=\"https://medium.com/@renjietan?source=rss-fdf264797c2a------2#bde1\">Primer</a></li>\n<li><a href=\"https://medium.com/@renjietan?source=rss-fdf264797c2a------2#f9ce\">AP and mAP for Information Retrieval</a></li>\n<li><a href=\"https://medium.com/@renjietan?source=rss-fdf264797c2a------2#1a59\">AP and mAP for Object Detection</a></li>\n</ol>\n<h3>1. Primer</h3>\n<p>Precision and recall are two commonly used metric to judge the performance of a given classification model. To understand mAP, we would need to first review precision and\u00a0recall.</p>\n<h4>The more \u201cfamous\u201d Precision and\u00a0Recall</h4>\n<p>In the field of statistics and Data Science, <strong>precision of a given class in classification, a.k.a. positive predicted value</strong>, is given as the ratio of true positive (TP) and the total number of predicted positives. The formula is given as\u00a0such:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/159/1*jiqAWT_Yzi_1LRD74dz-9Q.png\"><figcaption>Precision formula of a given class in classification</figcaption></figure><p>Similarly, the <strong>recall, a.k.a. true positive rate or sensitivity, of a given class in classification,</strong> is defined as the ratio of TP and total of ground truth positives. The formula is given as\u00a0such:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/185/1*ikwX1H72KPj1fUBppUWrpA.png\"><figcaption>Recall formula of a given class in classification</figcaption></figure><p>Just by looking at the formulas, we could suspect that for a given classification model, there lies a trade-off between its precision and recall performance. If we are using a neural network, this trade-off can be adjusted by the model\u2019s final layer softmax threshold.</p>\n<p>For our precision to be high, we would need to decrease our number of FP, by doing so, it will decrease our recall. Similarly, by decreasing our number of FN would increase our recall and decrease our precision. Very often for information retrieval and object detection cases, we would want our precision to be high (our predicted positives to be\u00a0TP).</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/350/0*WcR6weG1ORhMcJOl.png\"><figcaption>(<a href=\"https://en.wikipedia.org/wiki/Precision_and_recall\">source</a>)</figcaption></figure><p>Precision and recall are commonly used along with other metrics such as accuracy, F1-score, specificity, a.k.a. true negative rate (TNR), receiver operating characteristics (ROC), lift and\u00a0gain.</p>\n<h4>The less \u201cfamous\u201d Precision and\u00a0Recall</h4>\n<p>However, when it comes to information retrieval, the definition is <strong>different</strong>.</p>\n<p>As defined by Wiki, <strong>precision is defined as the ratio of the retrived documents that are relevant to user\u2019s query over the retrieved documents</strong>.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/573/1*rHehemm-D_OMPRDN2zJ-ZA.png\"><figcaption>Precision formula for information retrieval given by\u00a0Wiki</figcaption></figure><p>Keeping the nomenclature similar to formula defined above, a relevant document can be regarded as a\u00a0TP.</p>\n<p>By default, precision takes all the retrieved documents into account, but however, it can also be evaluated at a given number of retrieved documents, commonly known as cut-off rank, where the model is only assessed by considering only its top-most queries. The measure is called <em>precision at k or\u00a0</em>P@K.</p>\n<p>Let us use an example to better understand the\u00a0formula.</p>\n<h4>Defining a typical information retrieval task</h4>\n<p>A typical task in information retrieval is for a user to provide a query to a database and retrieving information very similar to the query. Let\u2019s now perform a calculation for precision with an example with <strong>three</strong> ground truth positives (GTP).</p>\n<p><em>Additional nomenclature: Ground truth positives are the labeled-as-positive data. In other words, the relevant documents.</em></p>\n<p>We shall define the following variables:</p>\n<ul>\n<li>Q to be the user\u00a0query</li>\n<li>G to be a set of labeled data in the\u00a0database</li>\n<li>d(i,j) to be a score function to show how similar object i is to\u00a0j</li>\n<li>G\u2019 which an ordered set of G according to score function d(\u00a0,\u00a0)</li>\n<li>k to be the index of\u00a0G\u2019</li>\n</ul>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/918/1*OdcEzsFJ3C-slus0OOBcwg.jpeg\"><figcaption>User querying G with a document\u00a0Q</figcaption></figure><p>After calculating the d(\u00a0, ) for each of the documents with Q, we can sort G and get\u00a0G\u2019.</p>\n<p>Say the model returns the following G\u2019</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*6iqevWZQH3Jqr6TQWkj21Q.png\"><figcaption>Model returned sorted query results\u00a0G\u2019</figcaption></figure><p>Using the Precision formula above, we get the following,</p>\n<p>P@1 = 1/1 =\u00a01</p>\n<p>P@2 = 1/2 =\u00a00.5</p>\n<p>P@3 = 1/3 =\u00a00.33</p>\n<p>P@4 = 2/4 =\u00a00.5</p>\n<p>P@5 = 3/5 =\u00a00.6</p>\n<p>P@n =\u00a03/n</p>\n<p>Similarly, the <strong>recall formula defined by Wiki is given as the ratio retrieved documents that are relevant to user\u2019s query over the relevant documents</strong>.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/532/1*OxT1NnGepMDtssqp6iVLsw.png\"><figcaption>Recall formula for information retrieval given by\u00a0Wiki</figcaption></figure><p>Recall in this case is not as useful as by running returning all the documents for a query will result in a trivial 100% recall, hence recall by itself is commonly not used as a\u00a0metric.</p>\n<p><a href=\"https://medium.com/@renjietan?source=rss-fdf264797c2a------2#d439\">Back to\u00a0outline</a></p>\n<h3>2. Average Precision and mAP for Information Retrieval</h3>\n<p>Being familiar with precision@k, we can now move on to calculate the Average Precision. This would give us a better measurement of our model in its ability to sorting the results of the query,\u00a0G\u2019.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/252/0*S5iua0fuLzHhdfPb\"><figcaption>AP@n formula</figcaption></figure><p>where GTP refers to the total number of ground truth positives, n refers to the total number of documents you are interested in, P@k refers to the precision@k and rel@k is a relevance function. The relevance function is an indicator function which equals 1 if the document at rank k is relevant and equals to 0 otherwise.</p>\n<p>Recall the definition of precision, we shall now use it to calculate the AP for each document in G\u2019. Using the same example shown\u00a0above,</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*nvVX0Hcp2ldZomsAYe_HAg.png\"><figcaption>Calculation of AP for a given query, Q, with a\u00a0GTP=3</figcaption></figure><p>The overall AP for this query is 0.7. One thing to note is that since we know that there are only three GTP, the AP@5 would equal to overall\u00a0AP.</p>\n<p>For another query, Q, we could get a perfect AP of 1 if the returned G\u2019 is sorted as\u00a0such:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*fPWfLU-3eBCbLZgSQaHJUw.png\"><figcaption>Calculation of a perfect AP for a given query, Q, with a\u00a0GTP=3</figcaption></figure><p>What AP does, in this case, is to penalise models that are not able to sort G\u2019 with TPs leading the set. It provides a number that is able to quantify the goodness of the sort based on the score function d(\u00a0, ). By dividing the sum of precision with the total GTP instead of dividing by the length of G\u2019 allows a better representation for queries that only have a few\u00a0GTP.</p>\n<h4>Calculating mAP</h4>\n<p>For each query, Q, we can calculate a corresponding AP. A user can have as much queries as he/she likes against this labeled database. The mAP is simply the mean of all the queries that the use\u00a0made.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/201/1*2U59KTw4gBF8pfUm2l53_g.png\"><figcaption>mAP formula for information retrieval</figcaption></figure><p><em>Note: This is the same formula as Wikipedia\u2019s, just written differently.</em></p>\n<p><a href=\"https://medium.com/@renjietan?source=rss-fdf264797c2a------2#d439\">Back to\u00a0outline</a></p>\n<h3>3. Average Precision and mAP for Object Detection</h3>\n<h4>Calculating AP (Traditional IoU =\u00a00.5)</h4>\n<h4>Intersection over Union\u00a0(IoU)</h4>\n<p>To do the calculation of AP for object detection, we would first need to understand IoU. The IoU is given by the ratio of the area of intersection and area of union of the predicted bounding box and ground truth bounding\u00a0box.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/600/0*mWSuiTMa6WyZUmyq.png\"><figcaption>(<a href=\"https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/\">source</a>)</figcaption></figure><p>The IoU would be used to determine if a predicted bounding box (BB) is TP, FP or FN. The TN is not evaluated as each image is assumed to have an object in it. Let us consider the following image\u00a0with:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/446/1*clVtJ9fRCzrmqxEOKwCGNQ.jpeg\"><figcaption>Image with a man and horse labeled with ground truth bounding boxes\u00a0(<a href=\"https://www.walmart.com/ip/LAMINATED-POSTER-Countryside-Horse-Mountains-Atlas-Man-Poster-Print-11-x-17/868208111\">source</a>)</figcaption></figure><p>The image contains a person and horse with their corresponding ground truth bounding boxes. Let us ignore the horse for the moment. We run our object detection model on this image and received a predicted bounding box for the person. Traditionally, we define a prediction to be a TP if the IoU is &gt; 0.5. The are possible scenarios are described below:</p>\n<h4>True Positive (IoU &gt;\u00a00.5)</h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/228/1*_efRgjUBc8AUj8XSB59hzg.jpeg\"><figcaption>IoU of predicted BB (yellow) and GT BB (blue) &gt; 0.5 with the correct classification</figcaption></figure><h4>False Positive</h4>\n<p>There are two possible scenarios where a BB would be considered as\u00a0FP:</p>\n<ul>\n<li>IoU &lt;\u00a00.5</li>\n<li>Duplicated BB</li>\n</ul>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/821/1*3Q4EfaCeCP2XWKTLk-hp6A.jpeg\"><figcaption>Illustrating the different scenarios a predicted BB (yellow) would be considered as\u00a0FP</figcaption></figure><h4>False Negative</h4>\n<p>When our object detection model missed the target, then it would be considered as a False Negative. The two possible scenarios are as\u00a0follows:</p>\n<ul><li>When there is no detection at\u00a0all.</li></ul>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/202/1*XPBq1HdtcPZiPP63D1nu3w.png\"><figcaption>No detection at\u00a0all</figcaption></figure><ul><li>When the predicted BB has an IoU &gt; 0.5 but has the wrong classification, the predicted BB would be\u00a0FN.</li></ul>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/228/1*cwX2KyTmCd0tR5P0mZbQdw.jpeg\"><figcaption>FN BB as the predicted class is a horse instead of a\u00a0person</figcaption></figure><h4>Precision/Recall Curve (PR\u00a0Curve)</h4>\n<p>With the TP, FP and FN formally defined, we can now calculate the precision and recall of our detection for a given class across the test set. Each BB would have its confidence level, usually given by its softmax layer, and would be used to rank the output. Note that this is very similar to the information retrieval case, just that instead of having a similarity function d(\u00a0, ) to provide the ranking, we used the model\u2019s predicted BB\u2019s confidence.</p>\n<h4>Interpolated precision</h4>\n<p>Before we plot the PR curve, we need first need to know the interpolated precision introduced in [<a href=\"http://host.robots.ox.ac.uk/pascal/VOC/pubs/everingham10.pdf\">1</a>]. The interpolated precision, p_interp, is calculated at each recall level, <em>r</em>, by taking the maximum precision measured for that <em>r.</em> The formula is given as\u00a0such:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/169/1*NpA3oCQA5AlKnJT7DlMhcQ.png\"><figcaption>Interpolated Precision for a given Recall Value\u00a0(r)</figcaption></figure><p>where p(r)\u02dc is the measured precision at recall\u00a0r\u02dc.</p>\n<p>Their intention of interpolating the PR curve was to reduce the impact of \u201cwiggles\u201d caused by small variations in the ranking of detections.</p>\n<p>With that out of the way, we can now start plotting the PR curve. Consider an example for the person class with 3 TP and 4 FP. We calculate the corresponding precision, recall, and interpolated precision given by the formulas defined\u00a0above.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/635/1*iV6bXUverNLvLI5uo2TlXA.png\"><figcaption>Calculation table for plotting PR curve with an example of 3 TP and 4 FP. Rows correspond to BB with person classification ordered by their respective softmax confidence</figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/769/1*Uhs5RacJtH3og8tvH5rctQ.png\"></figure><p>The AP is then calculated by taking the area under the PR curve. This is done by segmenting the recalls evenly to 11 parts: {0,0.1,0.2,\u2026,0.9,1}. We get the following:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/497/0*snOGkTye-01z6KWR\"><figcaption>Calculation of AP for the above\u00a0example</figcaption></figure><p>For another example, I would like to refer you to this <a href=\"https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173\">well-written article [2]</a> by <a href=\"https://medium.com/@jonathan_hui\">Jonathan\u00a0Hui</a>.</p>\n<h4>Calculating mAP</h4>\n<p>The mAP for object detection is the average of the AP calculated for all the classes. It is also important to note that for some papers, they use AP and mAP interchangeably.</p>\n<h4>Other ways of Calculating AP</h4>\n<p>With reference to [<a href=\"http://cocodataset.org/#detection-eval\">5</a>], COCO provided six new methods of calculating AP.</p>\n<p>Three of these methods are thresholding the BB at <strong>different IoUs:</strong></p>\n<ul>\n<li>AP: AP at IoU= 0.50: 0.05: 0.95 (primary challenge metric)</li>\n<li>AP@IoU=0.5 (traditional way of calculating as described above)</li>\n<li>AP@IoU=0.75 (IoU of BBs need to be &gt;\u00a00.75)</li>\n</ul>\n<p>For the primary AP, 0.5:0.05:0.95 means starting from IoU = 0.5, with steps of 0.05, we increase to an IoU = 0.95. These would result in computations of AP threshold at ten different IoUs. An average is done to provide a single number which rewards detectors that are better at localisation.</p>\n<p>The remaining three methods are calculating<strong> AP across\u00a0scales:</strong></p>\n<ul>\n<li>AP^small: AP for small objects: area &lt; 32\u00b2\u00a0px</li>\n<li>AP^medium: AP for medium objects: 32\u00b2 &lt; area &lt; 96\u00b2\u00a0px</li>\n<li>AP^large: AP for large objects: area &gt; 96\u00b2\u00a0px</li>\n</ul>\n<p>This would allow better differentiation of models as some datasets have more small objects than\u00a0others.</p>\n<p><a href=\"https://medium.com/@renjietan?source=rss-fdf264797c2a------2#d439\">Back to\u00a0outline</a></p>\n<p><em>Spot an error? Feel free to\u00a0comment!</em></p>\n<p><em>Special thanks to </em><a href=\"https://towardsdatascience.com/@raimibinkarim\"><em>Raimi</em></a><em>, </em><a href=\"https://medium.com/@derekchia\"><em>Derek</em></a><em> and </em><a href=\"https://medium.com/@polars\"><em>Wai Kit</em></a><em> for proofreading and giving me feedback on this\u00a0article.</em></p>\n<p><em>Feel free to connect with me via </em><a href=\"https://twitter.com/rahjaytee\"><em>Twitter</em></a><em>, </em><a href=\"https://www.linkedin.com/in/renjietan/\"><em>LinkedIn</em></a><em>!</em></p>\n<p><em>If you are interested in other projects that I have worked on, feel free to visit my\u00a0</em><a href=\"https://github.com/notha99y\"><em>Github</em></a><em>!</em></p>\n<p><em>For my other writings:</em></p>\n<ol>\n<li><a href=\"https://towardsdatascience.com/a-starter-pack-to-exploratory-data-analysis-with-python-pandas-seaborn-and-scikit-learn-a77889485baf\">A Starter Pack to Exploratory Data Analysis with Python, pandas, seaborn, and scikit-learn</a></li>\n<li><a href=\"https://towardsdatascience.com/6-steps-to-quickly-train-a-human-action-classifier-with-validation-accuracy-of-over-80-655fcb8781c5\">How I improved a Human Action Classifier to 80% Validation Accuracy in 6 Easy\u00a0Steps</a></li>\n</ol>\n<h3>References:</h3>\n<ol>\n<li>The PASCAL Visual Object Classes (VOC) Challenge, by Mark Everingham, Luc Van Gool, Christopher K. I. Williams, John Winn and Andrew Zisserman</li>\n<li><a href=\"https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173\">https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173</a></li>\n<li><a href=\"https://makarandtapaswi.wordpress.com/2012/07/02/intuition-behind-average-precision-and-map/\">https://makarandtapaswi.wordpress.com/2012/07/02/intuition-behind-average-precision-and-map/</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Mean_average_precision\">https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Mean_average_precision</a></li>\n<li><a href=\"http://cocodataset.org/#detection-eval\">http://cocodataset.org/#detection-eval</a></li>\n<li><a href=\"https://tarangshah.com/blog/2018-01-27/what-is-map-understanding-the-statistic-of-choice-for-comparing-object-detection-models/\">https://tarangshah.com/blog/2018-01-27/what-is-map-understanding-the-statistic-of-choice-for-comparing-object-detection-models/</a></li>\n</ol>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=ae462f623a52\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://towardsdatascience.com/breaking-down-mean-average-precision-map-ae462f623a52\">Breaking down Mean Average Precision (mAP)</a> was originally published in <a href=\"https://towardsdatascience.com/\">Towards Data Science</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","content":"\n<h3>Breaking Down Mean Average Precision (mAP)</h3>\n<h4>Another metric for your data science\u00a0toolkit</h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*OOD94B3vflKRs8yIBwWgQQ.jpeg\"><figcaption>Photo by <a href=\"https://unsplash.com/@byadonia?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">\u015eahin Ye\u015filyaprak</a> on\u00a0<a href=\"https://medium.com/s/photos/mean-average-precision?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Unsplash</a></figcaption></figure><p>If you have come across the <a href=\"http://host.robots.ox.ac.uk/pascal/VOC/index.html\">PASCAL Visual Object Classes (VOC)</a> and <a href=\"http://cocodataset.org/#home\">MS Common Objects in Context (COCO)</a> challenge, or dabbled with projects involving information retrieval and re-identification (ReID), you might then be quite familiar with a metric called\u00a0mAP.</p>\n<p>The mean average precision (mAP) or sometimes simply just referred to as AP is a popular metric used to measure the performance of models doing document/information retrival and object detection tasks.</p>\n<p>The mean average precision (mAP) of a set of queries is defined by <a href=\"https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Mean_average_precision\">Wikipedia</a> as\u00a0such:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/245/1*0uDpXRwQU90HAQA1LkQMRQ.png\"><figcaption>Mean average precision formula given provided by Wikipedia</figcaption></figure><p>where <em>Q </em>is the number of queries in the set and AveP(q) is the average precision (AP) for a given query,\u00a0q.</p>\n<p>What the formula is essentially telling us is that, for a given query, q, we calculate its corresponding AP, and then the mean of the all these AP scores would give us a single number, called the mAP, which quantifies how good our model is at performing the\u00a0query.</p>\n<p>This definition baffles people (like me) who are just starting out in this field. I had questions like what is this set of queries? And what does AP mean? Is it just an average of the precision?</p>\n<p>This article hopes to address these questions and calculate mAP for both object detection and information retrieval tasks. This article would also be exploring why mAP is an appropriate and commonly used metric for information retrieval and object detection tasks.</p>\n<h3>Outline</h3>\n<ol>\n<li><a href=\"https://medium.com/@renjietan?source=rss-fdf264797c2a------2#bde1\">Primer</a></li>\n<li><a href=\"https://medium.com/@renjietan?source=rss-fdf264797c2a------2#f9ce\">AP and mAP for Information Retrieval</a></li>\n<li><a href=\"https://medium.com/@renjietan?source=rss-fdf264797c2a------2#1a59\">AP and mAP for Object Detection</a></li>\n</ol>\n<h3>1. Primer</h3>\n<p>Precision and recall are two commonly used metric to judge the performance of a given classification model. To understand mAP, we would need to first review precision and\u00a0recall.</p>\n<h4>The more \u201cfamous\u201d Precision and\u00a0Recall</h4>\n<p>In the field of statistics and Data Science, <strong>precision of a given class in classification, a.k.a. positive predicted value</strong>, is given as the ratio of true positive (TP) and the total number of predicted positives. The formula is given as\u00a0such:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/159/1*jiqAWT_Yzi_1LRD74dz-9Q.png\"><figcaption>Precision formula of a given class in classification</figcaption></figure><p>Similarly, the <strong>recall, a.k.a. true positive rate or sensitivity, of a given class in classification,</strong> is defined as the ratio of TP and total of ground truth positives. The formula is given as\u00a0such:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/185/1*ikwX1H72KPj1fUBppUWrpA.png\"><figcaption>Recall formula of a given class in classification</figcaption></figure><p>Just by looking at the formulas, we could suspect that for a given classification model, there lies a trade-off between its precision and recall performance. If we are using a neural network, this trade-off can be adjusted by the model\u2019s final layer softmax threshold.</p>\n<p>For our precision to be high, we would need to decrease our number of FP, by doing so, it will decrease our recall. Similarly, by decreasing our number of FN would increase our recall and decrease our precision. Very often for information retrieval and object detection cases, we would want our precision to be high (our predicted positives to be\u00a0TP).</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/350/0*WcR6weG1ORhMcJOl.png\"><figcaption>(<a href=\"https://en.wikipedia.org/wiki/Precision_and_recall\">source</a>)</figcaption></figure><p>Precision and recall are commonly used along with other metrics such as accuracy, F1-score, specificity, a.k.a. true negative rate (TNR), receiver operating characteristics (ROC), lift and\u00a0gain.</p>\n<h4>The less \u201cfamous\u201d Precision and\u00a0Recall</h4>\n<p>However, when it comes to information retrieval, the definition is <strong>different</strong>.</p>\n<p>As defined by Wiki, <strong>precision is defined as the ratio of the retrived documents that are relevant to user\u2019s query over the retrieved documents</strong>.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/573/1*rHehemm-D_OMPRDN2zJ-ZA.png\"><figcaption>Precision formula for information retrieval given by\u00a0Wiki</figcaption></figure><p>Keeping the nomenclature similar to formula defined above, a relevant document can be regarded as a\u00a0TP.</p>\n<p>By default, precision takes all the retrieved documents into account, but however, it can also be evaluated at a given number of retrieved documents, commonly known as cut-off rank, where the model is only assessed by considering only its top-most queries. The measure is called <em>precision at k or\u00a0</em>P@K.</p>\n<p>Let us use an example to better understand the\u00a0formula.</p>\n<h4>Defining a typical information retrieval task</h4>\n<p>A typical task in information retrieval is for a user to provide a query to a database and retrieving information very similar to the query. Let\u2019s now perform a calculation for precision with an example with <strong>three</strong> ground truth positives (GTP).</p>\n<p><em>Additional nomenclature: Ground truth positives are the labeled-as-positive data. In other words, the relevant documents.</em></p>\n<p>We shall define the following variables:</p>\n<ul>\n<li>Q to be the user\u00a0query</li>\n<li>G to be a set of labeled data in the\u00a0database</li>\n<li>d(i,j) to be a score function to show how similar object i is to\u00a0j</li>\n<li>G\u2019 which an ordered set of G according to score function d(\u00a0,\u00a0)</li>\n<li>k to be the index of\u00a0G\u2019</li>\n</ul>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/918/1*OdcEzsFJ3C-slus0OOBcwg.jpeg\"><figcaption>User querying G with a document\u00a0Q</figcaption></figure><p>After calculating the d(\u00a0, ) for each of the documents with Q, we can sort G and get\u00a0G\u2019.</p>\n<p>Say the model returns the following G\u2019</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*6iqevWZQH3Jqr6TQWkj21Q.png\"><figcaption>Model returned sorted query results\u00a0G\u2019</figcaption></figure><p>Using the Precision formula above, we get the following,</p>\n<p>P@1 = 1/1 =\u00a01</p>\n<p>P@2 = 1/2 =\u00a00.5</p>\n<p>P@3 = 1/3 =\u00a00.33</p>\n<p>P@4 = 2/4 =\u00a00.5</p>\n<p>P@5 = 3/5 =\u00a00.6</p>\n<p>P@n =\u00a03/n</p>\n<p>Similarly, the <strong>recall formula defined by Wiki is given as the ratio retrieved documents that are relevant to user\u2019s query over the relevant documents</strong>.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/532/1*OxT1NnGepMDtssqp6iVLsw.png\"><figcaption>Recall formula for information retrieval given by\u00a0Wiki</figcaption></figure><p>Recall in this case is not as useful as by running returning all the documents for a query will result in a trivial 100% recall, hence recall by itself is commonly not used as a\u00a0metric.</p>\n<p><a href=\"https://medium.com/@renjietan?source=rss-fdf264797c2a------2#d439\">Back to\u00a0outline</a></p>\n<h3>2. Average Precision and mAP for Information Retrieval</h3>\n<p>Being familiar with precision@k, we can now move on to calculate the Average Precision. This would give us a better measurement of our model in its ability to sorting the results of the query,\u00a0G\u2019.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/252/0*S5iua0fuLzHhdfPb\"><figcaption>AP@n formula</figcaption></figure><p>where GTP refers to the total number of ground truth positives, n refers to the total number of documents you are interested in, P@k refers to the precision@k and rel@k is a relevance function. The relevance function is an indicator function which equals 1 if the document at rank k is relevant and equals to 0 otherwise.</p>\n<p>Recall the definition of precision, we shall now use it to calculate the AP for each document in G\u2019. Using the same example shown\u00a0above,</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*nvVX0Hcp2ldZomsAYe_HAg.png\"><figcaption>Calculation of AP for a given query, Q, with a\u00a0GTP=3</figcaption></figure><p>The overall AP for this query is 0.7. One thing to note is that since we know that there are only three GTP, the AP@5 would equal to overall\u00a0AP.</p>\n<p>For another query, Q, we could get a perfect AP of 1 if the returned G\u2019 is sorted as\u00a0such:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*fPWfLU-3eBCbLZgSQaHJUw.png\"><figcaption>Calculation of a perfect AP for a given query, Q, with a\u00a0GTP=3</figcaption></figure><p>What AP does, in this case, is to penalise models that are not able to sort G\u2019 with TPs leading the set. It provides a number that is able to quantify the goodness of the sort based on the score function d(\u00a0, ). By dividing the sum of precision with the total GTP instead of dividing by the length of G\u2019 allows a better representation for queries that only have a few\u00a0GTP.</p>\n<h4>Calculating mAP</h4>\n<p>For each query, Q, we can calculate a corresponding AP. A user can have as much queries as he/she likes against this labeled database. The mAP is simply the mean of all the queries that the use\u00a0made.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/201/1*2U59KTw4gBF8pfUm2l53_g.png\"><figcaption>mAP formula for information retrieval</figcaption></figure><p><em>Note: This is the same formula as Wikipedia\u2019s, just written differently.</em></p>\n<p><a href=\"https://medium.com/@renjietan?source=rss-fdf264797c2a------2#d439\">Back to\u00a0outline</a></p>\n<h3>3. Average Precision and mAP for Object Detection</h3>\n<h4>Calculating AP (Traditional IoU =\u00a00.5)</h4>\n<h4>Intersection over Union\u00a0(IoU)</h4>\n<p>To do the calculation of AP for object detection, we would first need to understand IoU. The IoU is given by the ratio of the area of intersection and area of union of the predicted bounding box and ground truth bounding\u00a0box.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/600/0*mWSuiTMa6WyZUmyq.png\"><figcaption>(<a href=\"https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/\">source</a>)</figcaption></figure><p>The IoU would be used to determine if a predicted bounding box (BB) is TP, FP or FN. The TN is not evaluated as each image is assumed to have an object in it. Let us consider the following image\u00a0with:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/446/1*clVtJ9fRCzrmqxEOKwCGNQ.jpeg\"><figcaption>Image with a man and horse labeled with ground truth bounding boxes\u00a0(<a href=\"https://www.walmart.com/ip/LAMINATED-POSTER-Countryside-Horse-Mountains-Atlas-Man-Poster-Print-11-x-17/868208111\">source</a>)</figcaption></figure><p>The image contains a person and horse with their corresponding ground truth bounding boxes. Let us ignore the horse for the moment. We run our object detection model on this image and received a predicted bounding box for the person. Traditionally, we define a prediction to be a TP if the IoU is &gt; 0.5. The are possible scenarios are described below:</p>\n<h4>True Positive (IoU &gt;\u00a00.5)</h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/228/1*_efRgjUBc8AUj8XSB59hzg.jpeg\"><figcaption>IoU of predicted BB (yellow) and GT BB (blue) &gt; 0.5 with the correct classification</figcaption></figure><h4>False Positive</h4>\n<p>There are two possible scenarios where a BB would be considered as\u00a0FP:</p>\n<ul>\n<li>IoU &lt;\u00a00.5</li>\n<li>Duplicated BB</li>\n</ul>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/821/1*3Q4EfaCeCP2XWKTLk-hp6A.jpeg\"><figcaption>Illustrating the different scenarios a predicted BB (yellow) would be considered as\u00a0FP</figcaption></figure><h4>False Negative</h4>\n<p>When our object detection model missed the target, then it would be considered as a False Negative. The two possible scenarios are as\u00a0follows:</p>\n<ul><li>When there is no detection at\u00a0all.</li></ul>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/202/1*XPBq1HdtcPZiPP63D1nu3w.png\"><figcaption>No detection at\u00a0all</figcaption></figure><ul><li>When the predicted BB has an IoU &gt; 0.5 but has the wrong classification, the predicted BB would be\u00a0FN.</li></ul>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/228/1*cwX2KyTmCd0tR5P0mZbQdw.jpeg\"><figcaption>FN BB as the predicted class is a horse instead of a\u00a0person</figcaption></figure><h4>Precision/Recall Curve (PR\u00a0Curve)</h4>\n<p>With the TP, FP and FN formally defined, we can now calculate the precision and recall of our detection for a given class across the test set. Each BB would have its confidence level, usually given by its softmax layer, and would be used to rank the output. Note that this is very similar to the information retrieval case, just that instead of having a similarity function d(\u00a0, ) to provide the ranking, we used the model\u2019s predicted BB\u2019s confidence.</p>\n<h4>Interpolated precision</h4>\n<p>Before we plot the PR curve, we need first need to know the interpolated precision introduced in [<a href=\"http://host.robots.ox.ac.uk/pascal/VOC/pubs/everingham10.pdf\">1</a>]. The interpolated precision, p_interp, is calculated at each recall level, <em>r</em>, by taking the maximum precision measured for that <em>r.</em> The formula is given as\u00a0such:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/169/1*NpA3oCQA5AlKnJT7DlMhcQ.png\"><figcaption>Interpolated Precision for a given Recall Value\u00a0(r)</figcaption></figure><p>where p(r)\u02dc is the measured precision at recall\u00a0r\u02dc.</p>\n<p>Their intention of interpolating the PR curve was to reduce the impact of \u201cwiggles\u201d caused by small variations in the ranking of detections.</p>\n<p>With that out of the way, we can now start plotting the PR curve. Consider an example for the person class with 3 TP and 4 FP. We calculate the corresponding precision, recall, and interpolated precision given by the formulas defined\u00a0above.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/635/1*iV6bXUverNLvLI5uo2TlXA.png\"><figcaption>Calculation table for plotting PR curve with an example of 3 TP and 4 FP. Rows correspond to BB with person classification ordered by their respective softmax confidence</figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/769/1*Uhs5RacJtH3og8tvH5rctQ.png\"></figure><p>The AP is then calculated by taking the area under the PR curve. This is done by segmenting the recalls evenly to 11 parts: {0,0.1,0.2,\u2026,0.9,1}. We get the following:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/497/0*snOGkTye-01z6KWR\"><figcaption>Calculation of AP for the above\u00a0example</figcaption></figure><p>For another example, I would like to refer you to this <a href=\"https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173\">well-written article [2]</a> by <a href=\"https://medium.com/@jonathan_hui\">Jonathan\u00a0Hui</a>.</p>\n<h4>Calculating mAP</h4>\n<p>The mAP for object detection is the average of the AP calculated for all the classes. It is also important to note that for some papers, they use AP and mAP interchangeably.</p>\n<h4>Other ways of Calculating AP</h4>\n<p>With reference to [<a href=\"http://cocodataset.org/#detection-eval\">5</a>], COCO provided six new methods of calculating AP.</p>\n<p>Three of these methods are thresholding the BB at <strong>different IoUs:</strong></p>\n<ul>\n<li>AP: AP at IoU= 0.50: 0.05: 0.95 (primary challenge metric)</li>\n<li>AP@IoU=0.5 (traditional way of calculating as described above)</li>\n<li>AP@IoU=0.75 (IoU of BBs need to be &gt;\u00a00.75)</li>\n</ul>\n<p>For the primary AP, 0.5:0.05:0.95 means starting from IoU = 0.5, with steps of 0.05, we increase to an IoU = 0.95. These would result in computations of AP threshold at ten different IoUs. An average is done to provide a single number which rewards detectors that are better at localisation.</p>\n<p>The remaining three methods are calculating<strong> AP across\u00a0scales:</strong></p>\n<ul>\n<li>AP^small: AP for small objects: area &lt; 32\u00b2\u00a0px</li>\n<li>AP^medium: AP for medium objects: 32\u00b2 &lt; area &lt; 96\u00b2\u00a0px</li>\n<li>AP^large: AP for large objects: area &gt; 96\u00b2\u00a0px</li>\n</ul>\n<p>This would allow better differentiation of models as some datasets have more small objects than\u00a0others.</p>\n<p><a href=\"https://medium.com/@renjietan?source=rss-fdf264797c2a------2#d439\">Back to\u00a0outline</a></p>\n<p><em>Spot an error? Feel free to\u00a0comment!</em></p>\n<p><em>Special thanks to </em><a href=\"https://towardsdatascience.com/@raimibinkarim\"><em>Raimi</em></a><em>, </em><a href=\"https://medium.com/@derekchia\"><em>Derek</em></a><em> and </em><a href=\"https://medium.com/@polars\"><em>Wai Kit</em></a><em> for proofreading and giving me feedback on this\u00a0article.</em></p>\n<p><em>Feel free to connect with me via </em><a href=\"https://twitter.com/rahjaytee\"><em>Twitter</em></a><em>, </em><a href=\"https://www.linkedin.com/in/renjietan/\"><em>LinkedIn</em></a><em>!</em></p>\n<p><em>If you are interested in other projects that I have worked on, feel free to visit my\u00a0</em><a href=\"https://github.com/notha99y\"><em>Github</em></a><em>!</em></p>\n<p><em>For my other writings:</em></p>\n<ol>\n<li><a href=\"https://towardsdatascience.com/a-starter-pack-to-exploratory-data-analysis-with-python-pandas-seaborn-and-scikit-learn-a77889485baf\">A Starter Pack to Exploratory Data Analysis with Python, pandas, seaborn, and scikit-learn</a></li>\n<li><a href=\"https://towardsdatascience.com/6-steps-to-quickly-train-a-human-action-classifier-with-validation-accuracy-of-over-80-655fcb8781c5\">How I improved a Human Action Classifier to 80% Validation Accuracy in 6 Easy\u00a0Steps</a></li>\n</ol>\n<h3>References:</h3>\n<ol>\n<li>The PASCAL Visual Object Classes (VOC) Challenge, by Mark Everingham, Luc Van Gool, Christopher K. I. Williams, John Winn and Andrew Zisserman</li>\n<li><a href=\"https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173\">https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173</a></li>\n<li><a href=\"https://makarandtapaswi.wordpress.com/2012/07/02/intuition-behind-average-precision-and-map/\">https://makarandtapaswi.wordpress.com/2012/07/02/intuition-behind-average-precision-and-map/</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Mean_average_precision\">https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Mean_average_precision</a></li>\n<li><a href=\"http://cocodataset.org/#detection-eval\">http://cocodataset.org/#detection-eval</a></li>\n<li><a href=\"https://tarangshah.com/blog/2018-01-27/what-is-map-understanding-the-statistic-of-choice-for-comparing-object-detection-models/\">https://tarangshah.com/blog/2018-01-27/what-is-map-understanding-the-statistic-of-choice-for-comparing-object-detection-models/</a></li>\n</ol>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=ae462f623a52\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://towardsdatascience.com/breaking-down-mean-average-precision-map-ae462f623a52\">Breaking down Mean Average Precision (mAP)</a> was originally published in <a href=\"https://towardsdatascience.com/\">Towards Data Science</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","enclosure":{},"categories":["towards-data-science","machine-learning","information-retrieval","object-detection","data-science"]},{"title":"3 Life Lessons to Remember","pubDate":"2019-01-04 07:28:29","link":"https://medium.com/@renjietan/3-life-lessons-to-remember-4567186b63a7?source=rss-fdf264797c2a------2","guid":"https://medium.com/p/4567186b63a7","author":"Ren Jie Tan","thumbnail":"https://cdn-images-1.medium.com/max/1000/0*GlAd13hir31OJxdf","description":"\n<h4><em>I have no desire to suffer twice, in reality, and then in retrospect. -Sophocles, Oedipus\u00a0Rex</em></h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1000/0*GlAd13hir31OJxdf\"><figcaption>Image adapted from <a href=\"https://unsplash.com/photos/7F_FcLhrsds\">Fredrick Kearney\u00a0Jr</a></figcaption></figure><h3>Lesson 1: Never be too busy for your loved\u00a0ones</h3>\n<p>It is easy to get caught up in the high cadence and demands of everyday living. With everyone vying a bit of your time, it does sometimes seem like you don\u2019t even have time for yourself. Oh\u2026 How much would you give to just take a day off and lie down on your bed and just stare at the ceiling. In times like these, we often forget about the ones who truly cared about us and take their presence for\u00a0granted.</p>\n<p>We should never be too busy for our loved ones. Because, when the whole world is against you, they would be the only people who would stand by you. Such love needs to be treasured and shouldn\u2019t be sacrificed for anything else. Although time seems to be a very scarce resource, it is important that we set aside windows of quality time, with no phones, emails or social media and enjoy the presences of your loved ones. Tell them how much they mean to you and how appreciative you are. <strong><em>It is not about having time. It is about making\u00a0time.</em></strong></p>\n<blockquote>The key is not to prioritise your schedule, but to schedule your priorities.</blockquote>\n<blockquote>\u2013 Stephen\u00a0Covey</blockquote>\n<h3>Lesson 2: Focus your energy on what you can\u00a0control</h3>\n<p>When I received my first pay increment, I was dumbstruck. Already feeling the prejudice from the company, I spent my first year working really hard to impress, in hope for a pay adjustment. Finally, the day, which everyone one was waiting for, came. It was pay increment day! My boss went around giving everyone an envelope and encased in it was the letter which shows your salary increase. I was excited to see the fruits of my labour being translated into remuneration. The recent performance appraisal I got seems positive which adds to the\u00a0hype.</p>\n<p>\u201cRen Jie, thank you for your hard work in the past year,\u201d said my boss as he passed me the envelope which at the moment looked like it was glowing. I quickly took it and went to a corner to open it and to my dismay, I was dumbstruck. The increase could not even beat inflation. I was devastated. Immediately, I visualized my future being surrounded by dark clouds. I was lost and the knowledge that there is a high probability that I would get stuck at this pay grade did not\u00a0help.</p>\n<p>I started blaming everyone but myself for my predicament. My dad got quite a lot from me as he was one the person that supported the decision to join this company aside other offers I have received. My friendtor (friend + mentor), <a href=\"https://azoolwolf.wordpress.com/\">Fredrick</a> was also the receiving end of my endless complains and frustrations (Thank you for listening! =D). It was then that I slowly came to realize that no amount of complaining would get me out of this. I needed to man up and face reality. <strong><em>What complaining only does is to provide a momentary respite from the\u00a0pain</em></strong>.</p>\n<p>It is beyond us to change our performance grading or pay adjustment. It is beyond us to change the fact that there will always be someone that disapproves or dislikes the things you do. It is beyond us that sometimes things would not go according to plan and as my warrant officer would often say, \u201cJust Suck Thumb!\u201d Instead of lamenting on the \u201cwhat-if\u201ds and \u201cif-only\u201ds, we should take what life throws at us and improvise. Instead of wasting energy on blaming others for our own circumstance, we should take full responsibilities and channel the energy into doing more and being more. Like how Viktor E. Frankl put it, <strong><em>\u201cForces beyond your control can take away everything you possess except one thing, your freedom to choose how you will respond to the situation.\u201d</em></strong></p>\n<blockquote><em>\u201cFuijitora\u2026\u201d Warlord of Sea, Donquixote Doflamingo snarled. \u201cHe is an idiot Tsuru-san. It was his last chance. If he had sided with me and helped me get rid of all those brats, it wouldn\u2019t have ended up like this.\u201d \u201cI was holding the reins, the reins to the monsters of the world,\u201d Doflamingo declared, \u201cyou guys shouldn\u2019t have taken me down. I\u2019m sure you people will regret\u00a0it.\u201d</em></blockquote>\n<blockquote><em>\u201cDon\u2019t say such pathetic crap.\u201d Great Staff Officer and Vice Admiral of the Marine, Tsuru said with a cool face. Doflamingo\u2019s signature grin turned into a frown. \u201cThere is no \u201cIf.\u201d It\u2019s crazy to even think about it. This outcome is the only reality. You\u00a0lost.\u201d</em></blockquote>\n<blockquote><em>\u201cOh, how can I best you?\u201d Doflamingo then burst into laughter.</em></blockquote>\n<blockquote>- One of my <a href=\"https://www.youtube.com/watch?v=ZpCX3EoKfHs\">favourite scene</a> from One\u00a0Piece.</blockquote>\n<h3>Lesson 3: The key to ending procrastination is to\u00a0start</h3>\n<p>The <strong><em>first 5 mins of doing meaningful things often suck!</em></strong> Whether it is heading to the gym, waking up to a chilly morning for a run or starting a new project, our brain would find a thousand and one excuses to put doing these stuff for later. It is hardwired in us. Anything which causes discomfort to our brain, our brain would naturally tend to avoid and look to do things which brings pleasure such as scrolling through your social media newsfeed and watching youtube\u00a0videos.</p>\n<p>It is only with willpower or the stress when realizing that the deadline for a certain project is approaching, do we pick up the courage and plow through the toughest 5 mins of our lives. Surprisingly, it is after the 5 minutes mark when the discomfort we feel start to fade away. We start to understand and remember the reason and motivation for why we are doing this in the first place. We start to gain traction and before we know it, we are in the zone. As Kenny from urban dictionary aptly described it, <strong><em>the zone is an expression used to describe a state of consciousness where actual skills match the perceived performance requirements perfectly</em></strong>. Being in the zone implies increased focus and attention which allow for higher levels of performance. Athletes, musicians, and anybody that totally owns a challenge of physical and mental performance can be in the\u00a0zone.</p>\n<p>To make full use of the limited time we have, it is paramount to fight through these annoying and difficult first 5 mins. Only by doing so can we retire to our beds feeling satisfied with the productive day we\u00a0had.</p>\n<p><em>This article was originally posted\u00a0</em><a href=\"https://notha99y.wordpress.com/2018/04/29/goodbye-st-3-lessons-to-take-away/\"><em>here</em></a><em>.</em></p>\n<p><em>If you\u2019ve enjoyed the read, please leave a comment or give me a clap or two!\u00a0=)</em></p>\n<p><em>Feel free to connect with me via </em><a href=\"https://twitter.com/rahjaytee\"><em>Twitter</em></a><em>, </em><a href=\"https://www.linkedin.com/in/renjietan/\"><em>LinkedIn</em></a><em>!</em></p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=4567186b63a7\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<h4><em>I have no desire to suffer twice, in reality, and then in retrospect. -Sophocles, Oedipus\u00a0Rex</em></h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1000/0*GlAd13hir31OJxdf\"><figcaption>Image adapted from <a href=\"https://unsplash.com/photos/7F_FcLhrsds\">Fredrick Kearney\u00a0Jr</a></figcaption></figure><h3>Lesson 1: Never be too busy for your loved\u00a0ones</h3>\n<p>It is easy to get caught up in the high cadence and demands of everyday living. With everyone vying a bit of your time, it does sometimes seem like you don\u2019t even have time for yourself. Oh\u2026 How much would you give to just take a day off and lie down on your bed and just stare at the ceiling. In times like these, we often forget about the ones who truly cared about us and take their presence for\u00a0granted.</p>\n<p>We should never be too busy for our loved ones. Because, when the whole world is against you, they would be the only people who would stand by you. Such love needs to be treasured and shouldn\u2019t be sacrificed for anything else. Although time seems to be a very scarce resource, it is important that we set aside windows of quality time, with no phones, emails or social media and enjoy the presences of your loved ones. Tell them how much they mean to you and how appreciative you are. <strong><em>It is not about having time. It is about making\u00a0time.</em></strong></p>\n<blockquote>The key is not to prioritise your schedule, but to schedule your priorities.</blockquote>\n<blockquote>\u2013 Stephen\u00a0Covey</blockquote>\n<h3>Lesson 2: Focus your energy on what you can\u00a0control</h3>\n<p>When I received my first pay increment, I was dumbstruck. Already feeling the prejudice from the company, I spent my first year working really hard to impress, in hope for a pay adjustment. Finally, the day, which everyone one was waiting for, came. It was pay increment day! My boss went around giving everyone an envelope and encased in it was the letter which shows your salary increase. I was excited to see the fruits of my labour being translated into remuneration. The recent performance appraisal I got seems positive which adds to the\u00a0hype.</p>\n<p>\u201cRen Jie, thank you for your hard work in the past year,\u201d said my boss as he passed me the envelope which at the moment looked like it was glowing. I quickly took it and went to a corner to open it and to my dismay, I was dumbstruck. The increase could not even beat inflation. I was devastated. Immediately, I visualized my future being surrounded by dark clouds. I was lost and the knowledge that there is a high probability that I would get stuck at this pay grade did not\u00a0help.</p>\n<p>I started blaming everyone but myself for my predicament. My dad got quite a lot from me as he was one the person that supported the decision to join this company aside other offers I have received. My friendtor (friend + mentor), <a href=\"https://azoolwolf.wordpress.com/\">Fredrick</a> was also the receiving end of my endless complains and frustrations (Thank you for listening! =D). It was then that I slowly came to realize that no amount of complaining would get me out of this. I needed to man up and face reality. <strong><em>What complaining only does is to provide a momentary respite from the\u00a0pain</em></strong>.</p>\n<p>It is beyond us to change our performance grading or pay adjustment. It is beyond us to change the fact that there will always be someone that disapproves or dislikes the things you do. It is beyond us that sometimes things would not go according to plan and as my warrant officer would often say, \u201cJust Suck Thumb!\u201d Instead of lamenting on the \u201cwhat-if\u201ds and \u201cif-only\u201ds, we should take what life throws at us and improvise. Instead of wasting energy on blaming others for our own circumstance, we should take full responsibilities and channel the energy into doing more and being more. Like how Viktor E. Frankl put it, <strong><em>\u201cForces beyond your control can take away everything you possess except one thing, your freedom to choose how you will respond to the situation.\u201d</em></strong></p>\n<blockquote><em>\u201cFuijitora\u2026\u201d Warlord of Sea, Donquixote Doflamingo snarled. \u201cHe is an idiot Tsuru-san. It was his last chance. If he had sided with me and helped me get rid of all those brats, it wouldn\u2019t have ended up like this.\u201d \u201cI was holding the reins, the reins to the monsters of the world,\u201d Doflamingo declared, \u201cyou guys shouldn\u2019t have taken me down. I\u2019m sure you people will regret\u00a0it.\u201d</em></blockquote>\n<blockquote><em>\u201cDon\u2019t say such pathetic crap.\u201d Great Staff Officer and Vice Admiral of the Marine, Tsuru said with a cool face. Doflamingo\u2019s signature grin turned into a frown. \u201cThere is no \u201cIf.\u201d It\u2019s crazy to even think about it. This outcome is the only reality. You\u00a0lost.\u201d</em></blockquote>\n<blockquote><em>\u201cOh, how can I best you?\u201d Doflamingo then burst into laughter.</em></blockquote>\n<blockquote>- One of my <a href=\"https://www.youtube.com/watch?v=ZpCX3EoKfHs\">favourite scene</a> from One\u00a0Piece.</blockquote>\n<h3>Lesson 3: The key to ending procrastination is to\u00a0start</h3>\n<p>The <strong><em>first 5 mins of doing meaningful things often suck!</em></strong> Whether it is heading to the gym, waking up to a chilly morning for a run or starting a new project, our brain would find a thousand and one excuses to put doing these stuff for later. It is hardwired in us. Anything which causes discomfort to our brain, our brain would naturally tend to avoid and look to do things which brings pleasure such as scrolling through your social media newsfeed and watching youtube\u00a0videos.</p>\n<p>It is only with willpower or the stress when realizing that the deadline for a certain project is approaching, do we pick up the courage and plow through the toughest 5 mins of our lives. Surprisingly, it is after the 5 minutes mark when the discomfort we feel start to fade away. We start to understand and remember the reason and motivation for why we are doing this in the first place. We start to gain traction and before we know it, we are in the zone. As Kenny from urban dictionary aptly described it, <strong><em>the zone is an expression used to describe a state of consciousness where actual skills match the perceived performance requirements perfectly</em></strong>. Being in the zone implies increased focus and attention which allow for higher levels of performance. Athletes, musicians, and anybody that totally owns a challenge of physical and mental performance can be in the\u00a0zone.</p>\n<p>To make full use of the limited time we have, it is paramount to fight through these annoying and difficult first 5 mins. Only by doing so can we retire to our beds feeling satisfied with the productive day we\u00a0had.</p>\n<p><em>This article was originally posted\u00a0</em><a href=\"https://notha99y.wordpress.com/2018/04/29/goodbye-st-3-lessons-to-take-away/\"><em>here</em></a><em>.</em></p>\n<p><em>If you\u2019ve enjoyed the read, please leave a comment or give me a clap or two!\u00a0=)</em></p>\n<p><em>Feel free to connect with me via </em><a href=\"https://twitter.com/rahjaytee\"><em>Twitter</em></a><em>, </em><a href=\"https://www.linkedin.com/in/renjietan/\"><em>LinkedIn</em></a><em>!</em></p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=4567186b63a7\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["retrospectives","life-lessons","reflections"]},{"title":"A Starter Pack to Exploratory Data Analysis with Python, pandas, seaborn, and scikit-Learn","pubDate":"2018-12-23 13:51:20","link":"https://towardsdatascience.com/a-starter-pack-to-exploratory-data-analysis-with-python-pandas-seaborn-and-scikit-learn-a77889485baf?source=rss-fdf264797c2a------2","guid":"https://medium.com/p/a77889485baf","author":"Ren Jie Tan","thumbnail":"https://cdn-images-1.medium.com/max/1024/1*yIH5UC713MFJzoCs_ckCfg.jpeg","description":"\n<h3>A Starter Pack to Exploratory Data Analysis with Python, pandas, seaborn, and scikit-learn</h3>\n<h4>I\u2019m a backpack loaded up with things and knickknacks too. Anything that you might need I\u2019ve got inside for you.\u200a\u2014\u200aBackpack from Dora the Explorer\u00a0(<a href=\"https://www.retrojunk.com/content/child/quote/page/6735/dora-the-explorer\">source</a>)</h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*yIH5UC713MFJzoCs_ckCfg.jpeg\"></figure><p>Exploratory Data Analysis (EDA) is the bread and butter of anyone who deals with data. With information increasing by 2.5 quintillions bytes per day (<a href=\"https://www.forbes.com/sites/bernardmarr/2018/05/21/how-much-data-do-we-create-every-day-the-mind-blowing-stats-everyone-should-read/#73eb132a60ba\">Forbes, 2018</a>), the need for efficient EDA techniques is at its all-time\u00a0high.</p>\n<p>So where is this deluge coming from? The amount of useful information is almost certainly not increasing at such a rate. When we take a closer look, we would realize that most of this increase is <strong>contributed by noise</strong>. There are so many hypotheses to test, so many datasets to mine, but a relatively constant amount of objective truth. With most data scientists, their key objective is to able to <strong>distinguish the signal from the noise</strong>, and EDA is the main process to do\u00a0this.</p>\n<h3>Enter EDA</h3>\n<p>In this post, I shall introduce a Starter Pack to perform EDA on the <a href=\"https://www.kaggle.com/francksylla/titanic-machine-learning-from-disaster\">Titanic dataset</a> using popular Python packages: pandas, matplotlib, seaborn, and scikit-learn.</p>\n<p>For code reference, you can refer to my GitHub repository <a href=\"https://github.com/notha99y/EDA\">here</a>.</p>\n<h3>Data for Communication</h3>\n<p>Generically, we visualize data for two primary\u00a0reasons:</p>\n<ul>\n<li>To understand (EDA)</li>\n<li>To communicate</li>\n</ul>\n<p>In the last section, I shall share some useful dashboarding guidelines that would aid you in convey the results of your analysis clearly and effectively.</p>\n<h3>Outline:</h3>\n<ol>\n<li><a href=\"https://medium.com/@renjietan?source=rss-fdf264797c2a------2#89dd\">What is\u00a0Data</a></li>\n<li><a href=\"https://medium.com/@renjietan?source=rss-fdf264797c2a------2#249d\">Categorical Analysis</a></li>\n<li><a href=\"https://medium.com/@renjietan?source=rss-fdf264797c2a------2#3534\">Quantitative Analysis</a></li>\n<li><a href=\"https://medium.com/@renjietan?source=rss-fdf264797c2a------2#def5\">Clustering</a></li>\n<li><a href=\"https://medium.com/@renjietan?source=rss-fdf264797c2a------2#0bc0\">Feature Importance by Tree-based Estimators</a></li>\n<li><a href=\"https://medium.com/@renjietan?source=rss-fdf264797c2a------2#4de5\">Dashboarding Techniques</a></li>\n</ol>\n<h3>1. What is\u00a0Data</h3>\n<p>First and foremost, some theory. The word \u201cdata\u201d was first used to mean \u201ctransmissible and storable computer information\u201d in 1946 (<a href=\"https://www.etymonline.com/word/data\">source</a>). At the highest level, the term data can be broadly categorized under two umbrellas: <strong>structured</strong> and <strong>unstructured</strong>. Structured data are pre-defined data models that normally reside in your relational database or data warehouse that has a fixed schema. Common examples include transaction information, customers\u2019 information, and dates. On the other hand, unstructured data have no pre-defined data model and are found in NoSQL databases and data lakes. Examples include images, video files, and audio\u00a0files.</p>\n<p>In this post, we would <strong>focus on structured data</strong>, where I would propose a systemic approach to quickly show latent statistics from your data. Under the umbrella of Structured data, we can further categorize them to <strong>categorical</strong> and <strong>quantitative</strong>. For Categorical data, the rules of arithmetics do not apply. In the categorical family, we have <strong>nominal</strong> and <strong>ordinal</strong> data, while in the Quantitative family, we have <strong>interval</strong> and <strong>ratio</strong>. It is important that we take some time to clearly define and understand the subtle, yet significant differences each term is from the other as this would affect our analysis and preprocessing techniques later.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/874/1*MpcJVoVMQimjUiLzi33izA.png\"><figcaption>4 Different types of\u00a0Data</figcaption></figure><h4>Nominal data</h4>\n<p>The name \u201cnominal\u201d comes from the Latin word, <em>nomen</em>, which means name. Nominal data are objects which are differentiated by a <strong>simple naming system</strong>. An important thing to note is that nominal data may also have numbers assigned to them. This may appear ordinal (definition below), but they are not. Numbered nominal data are simply used to capture and reference. Some examples\u00a0include:</p>\n<ul>\n<li>a set of countries.</li>\n<li>the number pinned on a marathon\u00a0runner.</li>\n</ul>\n<h4>Ordinal data</h4>\n<p>Ordinal data are items in which their <strong>order matters</strong>. More formally, their relative positions on an ordinal scale provide meaning to us. This may indicate superiority or temporal positions, etc.. By default, the order of ordinal data is defined by assigning numbers to them. However, letters or other sequential symbols may also be used as appropriate. Some examples\u00a0include:</p>\n<ul>\n<li>the competition ranking of a race (1st, 2nd,\u00a03rd)</li>\n<li>the salary grade in an organization (Associate, AVP, VP,\u00a0SVP).</li>\n</ul>\n<h4>Interval data</h4>\n<p>Similar to ordinal data, interval data is measured along a scale in which each object\u2019s position is <strong>equidistant</strong> from one another. This unique property allows arithmetic to be applied to them. An example\u00a0is</p>\n<ul><li>the temperature in degrees Fahrenheit where the difference between 78 degrees and 79 degrees is the same as 45 degrees and 46\u00a0degrees.</li></ul>\n<h4>Ratio data</h4>\n<p>Like Interval data, the differences in Ratio data are meaningful. The Ratio data has an added feature which makes the ratios of the objects meaningful as well, and that is that they have a <strong>true zero point</strong>. Zero represents the absence of a certain property. So when we say something is of zero weight, we mean that thing has an absence of mass. Some examples\u00a0include:</p>\n<ul><li>the weight of a person on a weighing\u00a0scale</li></ul>\n<p><strong>Interval vs\u00a0Ratio</strong></p>\n<p>The difference between interval and ratio is just one does not have a true zero point while the other does have. This is best illustrated with an example: When we say something is 0 degrees Fahrenheit, it does not mean an absence of heat in that thing. This unique property makes statements that involve ratios such as \u201c80 degrees Fahrenheit is twice as hot as 40 degrees Fahrenheit\u201d not hold\u00a0true.</p>\n<p><a href=\"https://medium.com/@renjietan?source=rss-fdf264797c2a------2#4603\"><em>Back to\u00a0outline</em></a></p>\n<p>Before we delve into the other sections, I would like to formalize some concepts so you would be clear on the thinking process to why we are doing things shown\u00a0below.</p>\n<p>Let me begin by saying that the best way to quickly show summaries of your data is through 2D plots. Despite living in a 3D spatial world, humans find it difficult to perceive the 3rd dimension, e.g. depth, needless to say, a projection of a 3D plot on a 2D screen. Hence, in the subsequent sections, you would see that we only use <strong>bar graphs</strong> for Categorical data, and <strong>box plots</strong> for Quantitative data as they succinctly express the data distributions respectively. We would only be focusing on <strong>univariate analysis</strong> and <strong>bivariate analysis</strong> with the target variable. For more details on Dashboarding Techniques, please refer to Section 6 of the\u00a0article.</p>\n<p>We would be mainly using <strong>seaborn</strong> and <strong>pandas</strong> to accomplish this. As we all know, statistics is an essential part of any data scientist\u2019s toolkit and seaborn allows quick and easy use of matplotlib to beautifully visualize the statistics of your data. matplotlib is powerful, but it can get complicated at times. Seaborn provides a high-level abstraction of matplotlib allowing us to <strong>plot attractive statistical plots with ease</strong>. To make the best use of seaborn, we would also need pandas as <strong>seaborn works best with pandas\u2019 DataFrames</strong>.</p>\n<p>Also, if you want to follow along with the coding, be sure to download the <a href=\"https://www.kaggle.com/francksylla/titanic-machine-learning-from-disaster/data\">data</a> and set up your environment right. You can find the instructions in the README.MD file in my GitHub\u00a0<a href=\"https://github.com/notha99y/EDA\">repo</a>.</p>\n<p>With these being said, let\u2019s begin and get our hands\u00a0dirty!</p>\n<h3>2. Categorical Analysis</h3>\n<p>We can start reading the data using pd.read_csv()\u00a0. By doing a\u00a0.head() on the data frame, we could have a quick peek at the top 5 rows of our data. For those who are not familiar with pandas or the concept of a data frame, I would highly recommend spending half a day going through the following resources:</p>\n<ol>\n<li><a href=\"https://pandas.pydata.org/\">Pandas documentation</a></li>\n<li><a href=\"https://github.com/mobileink/data.frame/wiki/What-is-a-Data-Frame%3F\">What is a DataFrame?</a></li>\n</ol>\n<p>Other useful methods are\u00a0.desribe() and\u00a0.info() where the former would\u00a0show:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/657/1*_ylSngYBS8sJiOskks5Tpg.png\"><figcaption>Output of\u00a0.describe() method on a data\u00a0frame</figcaption></figure><p>And the latter would\u00a0show:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/348/1*QPyVg9RECarKxbu48kiKtA.png\"><figcaption>Output of\u00a0.info() method on a data\u00a0frame</figcaption></figure><p>We see now\u00a0that,</p>\n<p><strong>Categorical data:</strong></p>\n<ul>\n<li>PassengerId,</li>\n<li>Survived,</li>\n<li>Pclass,</li>\n<li>Name,</li>\n<li>Sex,</li>\n<li>Ticket,</li>\n<li>Cabin,</li>\n<li>and Embarked</li>\n</ul>\n<p><strong>while Qualitative data:</strong></p>\n<ul>\n<li>Age,</li>\n<li>SibSp,</li>\n<li>Parch,</li>\n<li>and Fare</li>\n</ul>\n<p>Now, with this knowledge and what we have learned in Section 1, let\u2019s write a custom helper function that can be used to handle most kinds of Categorical data (or at least attempt to) and give a quick summary of them. We shall do these with the help of some pandas methods and seaborn\u2019s\u00a0.countplot() method. The helper function is called categorical_summarized and is shown\u00a0below.</p>\n<a href=\"https://medium.com/media/012f807f25fa850bb6a88d10d6f28249/href\">https://medium.com/media/012f807f25fa850bb6a88d10d6f28249/href</a><p>What categorical_summarized does is it takes in a data frame, together with some input arguments and outputs the following:</p>\n<ol>\n<li>The count, mean, std, min, max, and quartiles for numerical data or the count, unique, top class, and frequency of the top class for non-numerical data.</li>\n<li>Class frequencies of the interested column, if verbose is set to\u00a0True</li>\n<li>Bar graph of the count of each class of the interested column</li>\n</ol>\n<p>Let\u2019s talk a little bit about the input arguments. x and y take in a str type which corresponds to the interested column that we want to investigate. Setting the name of the column to x would create a bar graph with the x-axis showing the different classes and their respective counts on the y-axis. Setting the name of the interested column to y would just flip the axes of the previous plot where the different classes would be on the y-axis with x-axis showing the count. By setting the hue to the target variable, which is Survived in this case, the function would show the dependency of the target variable w.r.t. the interested column. Some example codes to show the usage of categorical_summarized are shown\u00a0below:</p>\n<h4>Some examples using `categorical_summarized`</h4>\n<h4>Univariate Analysis</h4>\n<a href=\"https://medium.com/media/4562e4d4b7a371ad79d9f3f588c73ead/href\">https://medium.com/media/4562e4d4b7a371ad79d9f3f588c73ead/href</a><p>Would give the following:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/697/1*4m6PuPo_FlXv8DJpLi4JMg.png\"><figcaption>Output of categorical_summarized on Survival\u00a0Variable</figcaption></figure><h4>Bivariate Analysis</h4>\n<a href=\"https://medium.com/media/5e07d5cd4b259d3bc0a3d8bba4b18df2/href\">https://medium.com/media/5e07d5cd4b259d3bc0a3d8bba4b18df2/href</a><p>Would give the following:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/707/1*MVIEc4cJa-R_aEUkXPRuMg.png\"><figcaption>Output of categorical_summarized on Gender Variable with hue set to\u00a0Survived</figcaption></figure><p>For more examples, please refer to the Titanic.ipynb in the GitHub\u00a0repo.</p>\n<p><a href=\"https://medium.com/@renjietan?source=rss-fdf264797c2a------2#4603\"><em>Back to\u00a0outline</em></a></p>\n<h3>3. Quantitative Analysis</h3>\n<p>Now, we could technically use a bar graph for Quantitative data but it would generally be quite messy (you can try using categorical_summarized on the Age column, and you would see a messy plot of thin bins). A neater way would be to use a box plot, which displays the distribution based on a five number summary: minimum, Q1, median, Q3, and\u00a0maximum.</p>\n<p>The next helper function, called quantitative_summarized\u00a0, is defined\u00a0below:</p>\n<a href=\"https://medium.com/media/f88403b030b8bcf497eb230c535c7681/href\">https://medium.com/media/f88403b030b8bcf497eb230c535c7681/href</a><p>Similar to categorical_summarized\u00a0, quantitative_summarized takes in a data frame and some input arguments to output the latent statistics and also a box plot and swarm plot (if swarm was set to true\u00a0).</p>\n<p>quantitative_summarized can take in one Quantitative Variable and up to two Categorical Variables, where the Quantitative Variable has to be assigned to y and the other two Categorical Variables can be assigned to x and hue respectively. Some example codes to show the usage is shown\u00a0below:</p>\n<h4>Some examples using `quantitative_summarized`</h4>\n<h4>Univariate Analysis</h4>\n<a href=\"https://medium.com/media/87d4dda79f594dd342ca4197f543f35f/href\">https://medium.com/media/87d4dda79f594dd342ca4197f543f35f/href</a><p>Would give the following:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/391/1*pReaeQwJUxasknHo531eug.png\"><figcaption>Output of quantitative_summarized on Age\u00a0Variable</figcaption></figure><h4>Bivariate Analysis</h4>\n<a href=\"https://medium.com/media/6809a642c394ab64383008d363a17c47/href\">https://medium.com/media/6809a642c394ab64383008d363a17c47/href</a><p>Would give the following:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/391/1*LNqRPGm70lPlRqc8o1iKzg.png\"><figcaption>Output of quantitative_summarized on Age Variable with x set to\u00a0Survived</figcaption></figure><h4>Multivariate Analysis</h4>\n<a href=\"https://medium.com/media/0ae25be5d77cffeee905492f98a068a7/href\">https://medium.com/media/0ae25be5d77cffeee905492f98a068a7/href</a><p>Would give the following:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/392/1*PIUUZAXGreqoX6m__Om0Ug.png\"><figcaption>Output of quantitative_summarized on Age Variable with x set to Survived and hue set to\u00a0Pclass</figcaption></figure><p><strong>Correlation Analysis</strong></p>\n<p>Another popular quantitative analysis we could use is to find the correlation between our variables. Correlation is a way to understand the relationship between the variables in our dataset. For any pairs of variables we\u00a0have:</p>\n<ul>\n<li>\n<strong>Positive Correlation</strong>: both variables change in the same direction</li>\n<li>\n<strong>Neutral Correlation</strong>: No relationship in the change of the variables</li>\n<li>\n<strong>Negative Correlation</strong>: variables change in opposite directions</li>\n</ul>\n<p>The Pearson\u2019s correlation coefficient is commonly used to summarize the strength of the linear relationship between two sample variables. It is the normalization of the covariance between the two variables which is given\u00a0as,</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/238/0*VaKgak2trkYdMz-w\"><figcaption>Pearson\u2019s Correlation Coefficient Formula</figcaption></figure><p>A good way to visualize this is with a heatmap. We first drop the categorical variables and fill up the missing values of the remaining quantitative variables with their mode. Pandas DataFrames allow for an easy calculation of Pearson\u2019s CC with a\u00a0.corr method. The code can be found\u00a0below</p>\n<a href=\"https://medium.com/media/1c5f489cf19b37477aa8dedc06ce0407/href\">https://medium.com/media/1c5f489cf19b37477aa8dedc06ce0407/href</a><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/759/1*rQEQlyiHpQ2nam7IhKBzsQ.png\"><figcaption>Heatmap of Pearson\u2019s CC for Quantitative Variables</figcaption></figure><p>One thing useful about having this visualization is that we can not only see the relationships between variables, for example, there is a negative Pearson\u2019s CC between Fare and Pclass, we could avoid the phenomenon of <strong>multicollinearity</strong> from happening.</p>\n<p>Multicollinearity can adversely affect generalized linear models, such as logistic regression. It occurs when there are high correlations between variables, resulting in unstable estimates of regression coefficients. However, if you are planning to use a Decision Tree for your predictor, then you need not need to worry about\u00a0this.</p>\n<p>An interesting read about collinearity can be found\u00a0<a href=\"https://medium.com/future-vision/collinearity-what-it-means-why-its-bad-and-how-does-it-affect-other-models-94e1db984168\">here</a>.</p>\n<p>Correlation can help with data imputation, where correlated variables can be used to fill in the miss values of each other. Correlation can also indicate the presence of a causal relationship. With that being said, it is important to note that <a href=\"https://en.wikipedia.org/wiki/Correlation_does_not_imply_causation\">correlation does not imply causation</a>.</p>\n<p>For more examples, please refer to the Titanic.ipynb in the GitHub\u00a0repo.</p>\n<p><a href=\"https://medium.com/@renjietan?source=rss-fdf264797c2a------2#4603\"><em>Back to\u00a0outline</em></a></p>\n<h3>4. Clustering</h3>\n<h4>k-Means Clustering</h4>\n<p>k-means clustering belongs to the family of <strong>partitioning clustering</strong>. In Partition clustering, we have to specify the number of clusters, <em>k</em>, that we want. This can be done by picking out the \u201celbow\u201d point of an inertia plot shown below. <strong>A good choice of <em>k</em>, is one that is not too large and has a low inertia value</strong>. The inertia is a metric to measure the cluster quality by giving us an indicator of how tightly packed a cluster\u00a0is.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/392/1*RIQ2JkabVK07E9LrE675lg.png\"><figcaption>Inertia plot using K Means Clustering to select a suitable k\u00a0value</figcaption></figure><p>As K Means calculates the distance between the features to decide if the following observation belongs to a certain centroid, we have to preprocess our data by encoding our Categorical Variables and filling up missing values. A simple function to preprocess is shown\u00a0below.</p>\n<a href=\"https://medium.com/media/5b38fcb30ca4e9716676a8780d396482/href\">https://medium.com/media/5b38fcb30ca4e9716676a8780d396482/href</a><p>Now that we have treated our data, we have to perform <strong>feature scaling</strong> so that the distance calculated across the features can be compared. This is done easily via sklearn.preprocessing library. For code implementation, please refer to the Titanic.ipynb\u00a0. After running the k-means algorithm, where we set <em>k = 2</em>, we can plot the variables as shown\u00a0below.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/497/1*GN4o-UBZxatBILXZQYto8g.png\"><figcaption>Cluster red and blue plotted on Age and Survived\u00a0Axes</figcaption></figure><h4>Hierarchical Agglomerative Clustering</h4>\n<p>For this subsection, I would present another quick way to perform EDA via clustering. Agglomerative clustering uses a bottom-up approach where individual observations are joined together iteratively based on their distance. We would be using the scipy.cluster.hierarchy package to perform the linkages and show our results using a dendrogram. The distance between two clusters is calculated via the <strong>nearest neighbor\u00a0method</strong>.</p>\n<a href=\"https://medium.com/media/0b539f45e4566dd240147d3c91447c28/href\">https://medium.com/media/0b539f45e4566dd240147d3c91447c28/href</a><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/933/1*NYpmiX4PVC75OyFEUTabFg.png\"><figcaption>Dendrogram of the Hierarchical Clustering of the Titanic\u00a0Dataset</figcaption></figure><p><a href=\"https://medium.com/@renjietan?source=rss-fdf264797c2a------2#4603\"><em>Back to\u00a0outline</em></a></p>\n<h3>5. Feature Importance by Tree-based estimators</h3>\n<p>Another quick way to perform EDA is via tree-based estimators. A decision tree learns how to \u201cbest\u201d split the dataset into smaller subsets before finally outputting the predicted leaf node. The split is commonly defined by an <strong>impurity criterion</strong> like <strong>Gini</strong> or <strong>Information Gain Entropy</strong>. Since this is a post on EDA and not decision trees, I shall not explain the maths behind them in detail, but I shall show you how you could use them to understand your features better. You may refer to this <a href=\"https://medium.com/@srnghn/the-mathematics-of-decision-trees-random-forest-and-feature-importance-in-scikit-learn-and-spark-f2861df67e3\">well-written article</a> for more\u00a0details.</p>\n<p>Based on the impurity criterion, a tree can be built by greedily picking the features that contribute to the most information gain. To illustrate this, I shall use the scikit-learn library.</p>\n<h4>Build a Random Forest Classifier</h4>\n<p>We first build a random forest classifier. By default, the impurity criterion is set to Gini. Using the following code, we could see the corresponding feature importance of our Titanic\u00a0dataset.</p>\n<a href=\"https://medium.com/media/4728ebe8147ab73ec45d506322b006e0/href\">https://medium.com/media/4728ebe8147ab73ec45d506322b006e0/href</a><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/633/1*W98z_WNBSBa5fpzSlnMtJA.png\"></figure><h4>Let\u2019s try\u00a0XGBoost</h4>\n<p>Another way to create an ensemble of decision trees is via XGBoost, which is part of the family of gradient boosted framework. Using the following code, we could see which corresponding feature is important to our XGBoost. Similarly, by default, the impurity criterion is set to\u00a0Gini.</p>\n<a href=\"https://medium.com/media/97ad96a6056c9d2b76fa0f0f2be99ec1/href\">https://medium.com/media/97ad96a6056c9d2b76fa0f0f2be99ec1/href</a><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/633/1*ANVyGW9NW6I7VeahNjT1JA.png\"></figure><p><a href=\"https://medium.com/@renjietan?source=rss-fdf264797c2a------2#4603\"><em>Back to\u00a0outline</em></a></p>\n<h3>6. Dashboarding Techniques</h3>\n<p>Unlike infographics, dashboards are created to objectively display important information in a clean, concise manner on a single screen with the purpose to inform and not misguide its readers. Very often, dashboards are a representation of the data that exploit our visual perception abilities in order to amplify cognition. The information they display is of high graphical excellence and is understood by all, requiring no supplementary information for interpretation.</p>\n<p>To achieve Graphical Excellence, the following two key aspects have to be\u00a0obeyed:</p>\n<ol>\n<li>Maximize Data: Ink and Minimize Chartjunk</li>\n<li>Have High Graphical Integrity</li>\n</ol>\n<h4>Data: Ink &amp; Chartjunk</h4>\n<p>Data: Ink is defined as the ink used to represent data, while chartjunk is defined as the superfluous, decorative, or diverting ink. Examples of chartjunk are moir\u00e9 vibration, grids, and the duck. To achieve this, we make use of our <strong>pre-attentive attributes</strong> and <strong>Gestalt's Principles</strong> to bring out patterns in visualizations.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/443/1*pwJ9guekqUn1ItbxTiXu0w.png\"><figcaption>Image showing the side of a man\u2019s face, at the same time, the front of his face. An example to illustrate Figure &amp;\u00a0Ground</figcaption></figure><h4>Achieving Graphical Integrity</h4>\n<p>Adapted from: Tufte, Edward. (2001). <a href=\"https://www.amazon.com/Visual-Display-Quantitative-Information/dp/1930824130\">The Visual Display of Quantitative Information, 2nd Editions, Graphics</a>, there are six principles to ensure Graphical Integrity:</p>\n<ol>\n<li>Make the representation of numbers proportional to quantities</li>\n<li>Use clear, detailed, and thorough\u00a0labeling</li>\n<li>Show data variation, not design variation</li>\n<li>Use standardized units, not nominal\u00a0values</li>\n<li>Depict \u2019n\u2019 data dimensions with less than or equal to \u2019n\u2019 variable dimensions</li>\n<li>Quote data in full\u00a0context</li>\n</ol>\n<p>As such, we avoid things like pie charts or putzing around with 3D graphs or with area dimensions. Bar graphs and box plots are such good examples to achieve graphical integrity as they are simple (everyone could understand without ambiguity) and powerful. It is also important to not miss out on context, like having the axis displaying quantitative data referenced to\u00a0zero.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*JMGUtWECuUve-cMtv4CbCg.png\"><figcaption>Examples of Misleading Graphics with Poor Graphical Integrity</figcaption></figure><p>Till date, <a href=\"https://www.tableau.com/\">Tableau</a> is probably the industry leader when it comes to dashboarding. They take in best dashboarding practices and also do the heavy lifting of plotting the graphs for you just by dragging and dropping. I would highly recommend anyone who has an interest in this field to take a look at Tableau. Another up and coming open-source project, which does similar things to Tableau, is called <a href=\"https://superset.incubator.apache.org/\">Apache Superset</a>.</p>\n<p><a href=\"https://medium.com/@renjietan?source=rss-fdf264797c2a------2#4603\"><em>Back to\u00a0outline</em></a></p>\n<h3>Conclusion</h3>\n<p>All in all, there are numerous ways to do EDA. EDA is part of the entire data science process, which is highly iterative. What I have shared in this article are things which I have picked up and stayed with me after joining a number of data science hackathons, my time as a data scientist in <a href=\"https://www.aisingapore.org/\">AI Singapore</a>, as well as my projects in my <a href=\"https://www.iss.nus.edu.sg/graduate-programmes/programme/detail/master-of-technology-in-knowledge-engineering\">MTech course</a>. I firmly believe in first establishing a firm foundation and then we optimize for speed and performance and hence, my motivation for writing this post. With this set of helper functions, my team and I were able to understand the latent statistics of our given dataset, and then develop a sound strategy to build a classifier within half a day of\u00a0hacking.</p>\n<p>This post is not meant to be a complete guide to EDA, but a Starter Pack to get aspiring Data Scientist kickoff their first data science projects using open-source tools like Python, pandas, seaborn, and scikit-learn. If you have any cool custom functions that are built upon Python packages and have aided you in your EDA process, please feel free to leave a comment below with your code. You could also submit a pull request and contribute to the helper.py file. Thanks for reading \ud83d\ude04. I hope you took away a thing or\u00a0two.</p>\n<p><em>Special mention to </em><a href=\"https://towardsdatascience.com/@raimibinkarim\"><em>Raimi</em></a><em>, </em><a href=\"https://towardsdatascience.com/@derekchia\"><em>Derek</em></a><em>, and </em><a href=\"https://medium.com/@cykoay04157\"><em>Chin Yang</em></a><em> for proofreading and giving me feedback on improving this\u00a0article</em></p>\n<p><em>Feel free to connect with me via </em><a href=\"https://twitter.com/rahjaytee\"><em>Twitter</em></a><em>, </em><a href=\"https://www.linkedin.com/in/renjietan/\"><em>LinkedIn</em></a><em>!</em></p>\n<p><em>If you are interested in the other project that I have worked on, feel free to visit my\u00a0</em><a href=\"https://github.com/notha99y\"><em>GitHub</em></a><em>!</em></p>\n<p><em>Want to learn how to build a Human Action Classifier quickly? Do check out my previous post over\u00a0</em><a href=\"https://towardsdatascience.com/6-steps-to-quickly-train-a-human-action-classifier-with-validation-accuracy-of-over-80-655fcb8781c5\"><em>here</em></a><em>.</em></p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=a77889485baf\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://towardsdatascience.com/a-starter-pack-to-exploratory-data-analysis-with-python-pandas-seaborn-and-scikit-learn-a77889485baf\">A Starter Pack to Exploratory Data Analysis with Python, pandas, seaborn, and scikit-Learn</a> was originally published in <a href=\"https://towardsdatascience.com/\">Towards Data Science</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","content":"\n<h3>A Starter Pack to Exploratory Data Analysis with Python, pandas, seaborn, and scikit-learn</h3>\n<h4>I\u2019m a backpack loaded up with things and knickknacks too. Anything that you might need I\u2019ve got inside for you.\u200a\u2014\u200aBackpack from Dora the Explorer\u00a0(<a href=\"https://www.retrojunk.com/content/child/quote/page/6735/dora-the-explorer\">source</a>)</h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*yIH5UC713MFJzoCs_ckCfg.jpeg\"></figure><p>Exploratory Data Analysis (EDA) is the bread and butter of anyone who deals with data. With information increasing by 2.5 quintillions bytes per day (<a href=\"https://www.forbes.com/sites/bernardmarr/2018/05/21/how-much-data-do-we-create-every-day-the-mind-blowing-stats-everyone-should-read/#73eb132a60ba\">Forbes, 2018</a>), the need for efficient EDA techniques is at its all-time\u00a0high.</p>\n<p>So where is this deluge coming from? The amount of useful information is almost certainly not increasing at such a rate. When we take a closer look, we would realize that most of this increase is <strong>contributed by noise</strong>. There are so many hypotheses to test, so many datasets to mine, but a relatively constant amount of objective truth. With most data scientists, their key objective is to able to <strong>distinguish the signal from the noise</strong>, and EDA is the main process to do\u00a0this.</p>\n<h3>Enter EDA</h3>\n<p>In this post, I shall introduce a Starter Pack to perform EDA on the <a href=\"https://www.kaggle.com/francksylla/titanic-machine-learning-from-disaster\">Titanic dataset</a> using popular Python packages: pandas, matplotlib, seaborn, and scikit-learn.</p>\n<p>For code reference, you can refer to my GitHub repository <a href=\"https://github.com/notha99y/EDA\">here</a>.</p>\n<h3>Data for Communication</h3>\n<p>Generically, we visualize data for two primary\u00a0reasons:</p>\n<ul>\n<li>To understand (EDA)</li>\n<li>To communicate</li>\n</ul>\n<p>In the last section, I shall share some useful dashboarding guidelines that would aid you in convey the results of your analysis clearly and effectively.</p>\n<h3>Outline:</h3>\n<ol>\n<li><a href=\"https://medium.com/@renjietan?source=rss-fdf264797c2a------2#89dd\">What is\u00a0Data</a></li>\n<li><a href=\"https://medium.com/@renjietan?source=rss-fdf264797c2a------2#249d\">Categorical Analysis</a></li>\n<li><a href=\"https://medium.com/@renjietan?source=rss-fdf264797c2a------2#3534\">Quantitative Analysis</a></li>\n<li><a href=\"https://medium.com/@renjietan?source=rss-fdf264797c2a------2#def5\">Clustering</a></li>\n<li><a href=\"https://medium.com/@renjietan?source=rss-fdf264797c2a------2#0bc0\">Feature Importance by Tree-based Estimators</a></li>\n<li><a href=\"https://medium.com/@renjietan?source=rss-fdf264797c2a------2#4de5\">Dashboarding Techniques</a></li>\n</ol>\n<h3>1. What is\u00a0Data</h3>\n<p>First and foremost, some theory. The word \u201cdata\u201d was first used to mean \u201ctransmissible and storable computer information\u201d in 1946 (<a href=\"https://www.etymonline.com/word/data\">source</a>). At the highest level, the term data can be broadly categorized under two umbrellas: <strong>structured</strong> and <strong>unstructured</strong>. Structured data are pre-defined data models that normally reside in your relational database or data warehouse that has a fixed schema. Common examples include transaction information, customers\u2019 information, and dates. On the other hand, unstructured data have no pre-defined data model and are found in NoSQL databases and data lakes. Examples include images, video files, and audio\u00a0files.</p>\n<p>In this post, we would <strong>focus on structured data</strong>, where I would propose a systemic approach to quickly show latent statistics from your data. Under the umbrella of Structured data, we can further categorize them to <strong>categorical</strong> and <strong>quantitative</strong>. For Categorical data, the rules of arithmetics do not apply. In the categorical family, we have <strong>nominal</strong> and <strong>ordinal</strong> data, while in the Quantitative family, we have <strong>interval</strong> and <strong>ratio</strong>. It is important that we take some time to clearly define and understand the subtle, yet significant differences each term is from the other as this would affect our analysis and preprocessing techniques later.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/874/1*MpcJVoVMQimjUiLzi33izA.png\"><figcaption>4 Different types of\u00a0Data</figcaption></figure><h4>Nominal data</h4>\n<p>The name \u201cnominal\u201d comes from the Latin word, <em>nomen</em>, which means name. Nominal data are objects which are differentiated by a <strong>simple naming system</strong>. An important thing to note is that nominal data may also have numbers assigned to them. This may appear ordinal (definition below), but they are not. Numbered nominal data are simply used to capture and reference. Some examples\u00a0include:</p>\n<ul>\n<li>a set of countries.</li>\n<li>the number pinned on a marathon\u00a0runner.</li>\n</ul>\n<h4>Ordinal data</h4>\n<p>Ordinal data are items in which their <strong>order matters</strong>. More formally, their relative positions on an ordinal scale provide meaning to us. This may indicate superiority or temporal positions, etc.. By default, the order of ordinal data is defined by assigning numbers to them. However, letters or other sequential symbols may also be used as appropriate. Some examples\u00a0include:</p>\n<ul>\n<li>the competition ranking of a race (1st, 2nd,\u00a03rd)</li>\n<li>the salary grade in an organization (Associate, AVP, VP,\u00a0SVP).</li>\n</ul>\n<h4>Interval data</h4>\n<p>Similar to ordinal data, interval data is measured along a scale in which each object\u2019s position is <strong>equidistant</strong> from one another. This unique property allows arithmetic to be applied to them. An example\u00a0is</p>\n<ul><li>the temperature in degrees Fahrenheit where the difference between 78 degrees and 79 degrees is the same as 45 degrees and 46\u00a0degrees.</li></ul>\n<h4>Ratio data</h4>\n<p>Like Interval data, the differences in Ratio data are meaningful. The Ratio data has an added feature which makes the ratios of the objects meaningful as well, and that is that they have a <strong>true zero point</strong>. Zero represents the absence of a certain property. So when we say something is of zero weight, we mean that thing has an absence of mass. Some examples\u00a0include:</p>\n<ul><li>the weight of a person on a weighing\u00a0scale</li></ul>\n<p><strong>Interval vs\u00a0Ratio</strong></p>\n<p>The difference between interval and ratio is just one does not have a true zero point while the other does have. This is best illustrated with an example: When we say something is 0 degrees Fahrenheit, it does not mean an absence of heat in that thing. This unique property makes statements that involve ratios such as \u201c80 degrees Fahrenheit is twice as hot as 40 degrees Fahrenheit\u201d not hold\u00a0true.</p>\n<p><a href=\"https://medium.com/@renjietan?source=rss-fdf264797c2a------2#4603\"><em>Back to\u00a0outline</em></a></p>\n<p>Before we delve into the other sections, I would like to formalize some concepts so you would be clear on the thinking process to why we are doing things shown\u00a0below.</p>\n<p>Let me begin by saying that the best way to quickly show summaries of your data is through 2D plots. Despite living in a 3D spatial world, humans find it difficult to perceive the 3rd dimension, e.g. depth, needless to say, a projection of a 3D plot on a 2D screen. Hence, in the subsequent sections, you would see that we only use <strong>bar graphs</strong> for Categorical data, and <strong>box plots</strong> for Quantitative data as they succinctly express the data distributions respectively. We would only be focusing on <strong>univariate analysis</strong> and <strong>bivariate analysis</strong> with the target variable. For more details on Dashboarding Techniques, please refer to Section 6 of the\u00a0article.</p>\n<p>We would be mainly using <strong>seaborn</strong> and <strong>pandas</strong> to accomplish this. As we all know, statistics is an essential part of any data scientist\u2019s toolkit and seaborn allows quick and easy use of matplotlib to beautifully visualize the statistics of your data. matplotlib is powerful, but it can get complicated at times. Seaborn provides a high-level abstraction of matplotlib allowing us to <strong>plot attractive statistical plots with ease</strong>. To make the best use of seaborn, we would also need pandas as <strong>seaborn works best with pandas\u2019 DataFrames</strong>.</p>\n<p>Also, if you want to follow along with the coding, be sure to download the <a href=\"https://www.kaggle.com/francksylla/titanic-machine-learning-from-disaster/data\">data</a> and set up your environment right. You can find the instructions in the README.MD file in my GitHub\u00a0<a href=\"https://github.com/notha99y/EDA\">repo</a>.</p>\n<p>With these being said, let\u2019s begin and get our hands\u00a0dirty!</p>\n<h3>2. Categorical Analysis</h3>\n<p>We can start reading the data using pd.read_csv()\u00a0. By doing a\u00a0.head() on the data frame, we could have a quick peek at the top 5 rows of our data. For those who are not familiar with pandas or the concept of a data frame, I would highly recommend spending half a day going through the following resources:</p>\n<ol>\n<li><a href=\"https://pandas.pydata.org/\">Pandas documentation</a></li>\n<li><a href=\"https://github.com/mobileink/data.frame/wiki/What-is-a-Data-Frame%3F\">What is a DataFrame?</a></li>\n</ol>\n<p>Other useful methods are\u00a0.desribe() and\u00a0.info() where the former would\u00a0show:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/657/1*_ylSngYBS8sJiOskks5Tpg.png\"><figcaption>Output of\u00a0.describe() method on a data\u00a0frame</figcaption></figure><p>And the latter would\u00a0show:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/348/1*QPyVg9RECarKxbu48kiKtA.png\"><figcaption>Output of\u00a0.info() method on a data\u00a0frame</figcaption></figure><p>We see now\u00a0that,</p>\n<p><strong>Categorical data:</strong></p>\n<ul>\n<li>PassengerId,</li>\n<li>Survived,</li>\n<li>Pclass,</li>\n<li>Name,</li>\n<li>Sex,</li>\n<li>Ticket,</li>\n<li>Cabin,</li>\n<li>and Embarked</li>\n</ul>\n<p><strong>while Qualitative data:</strong></p>\n<ul>\n<li>Age,</li>\n<li>SibSp,</li>\n<li>Parch,</li>\n<li>and Fare</li>\n</ul>\n<p>Now, with this knowledge and what we have learned in Section 1, let\u2019s write a custom helper function that can be used to handle most kinds of Categorical data (or at least attempt to) and give a quick summary of them. We shall do these with the help of some pandas methods and seaborn\u2019s\u00a0.countplot() method. The helper function is called categorical_summarized and is shown\u00a0below.</p>\n<a href=\"https://medium.com/media/012f807f25fa850bb6a88d10d6f28249/href\">https://medium.com/media/012f807f25fa850bb6a88d10d6f28249/href</a><p>What categorical_summarized does is it takes in a data frame, together with some input arguments and outputs the following:</p>\n<ol>\n<li>The count, mean, std, min, max, and quartiles for numerical data or the count, unique, top class, and frequency of the top class for non-numerical data.</li>\n<li>Class frequencies of the interested column, if verbose is set to\u00a0True</li>\n<li>Bar graph of the count of each class of the interested column</li>\n</ol>\n<p>Let\u2019s talk a little bit about the input arguments. x and y take in a str type which corresponds to the interested column that we want to investigate. Setting the name of the column to x would create a bar graph with the x-axis showing the different classes and their respective counts on the y-axis. Setting the name of the interested column to y would just flip the axes of the previous plot where the different classes would be on the y-axis with x-axis showing the count. By setting the hue to the target variable, which is Survived in this case, the function would show the dependency of the target variable w.r.t. the interested column. Some example codes to show the usage of categorical_summarized are shown\u00a0below:</p>\n<h4>Some examples using `categorical_summarized`</h4>\n<h4>Univariate Analysis</h4>\n<a href=\"https://medium.com/media/4562e4d4b7a371ad79d9f3f588c73ead/href\">https://medium.com/media/4562e4d4b7a371ad79d9f3f588c73ead/href</a><p>Would give the following:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/697/1*4m6PuPo_FlXv8DJpLi4JMg.png\"><figcaption>Output of categorical_summarized on Survival\u00a0Variable</figcaption></figure><h4>Bivariate Analysis</h4>\n<a href=\"https://medium.com/media/5e07d5cd4b259d3bc0a3d8bba4b18df2/href\">https://medium.com/media/5e07d5cd4b259d3bc0a3d8bba4b18df2/href</a><p>Would give the following:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/707/1*MVIEc4cJa-R_aEUkXPRuMg.png\"><figcaption>Output of categorical_summarized on Gender Variable with hue set to\u00a0Survived</figcaption></figure><p>For more examples, please refer to the Titanic.ipynb in the GitHub\u00a0repo.</p>\n<p><a href=\"https://medium.com/@renjietan?source=rss-fdf264797c2a------2#4603\"><em>Back to\u00a0outline</em></a></p>\n<h3>3. Quantitative Analysis</h3>\n<p>Now, we could technically use a bar graph for Quantitative data but it would generally be quite messy (you can try using categorical_summarized on the Age column, and you would see a messy plot of thin bins). A neater way would be to use a box plot, which displays the distribution based on a five number summary: minimum, Q1, median, Q3, and\u00a0maximum.</p>\n<p>The next helper function, called quantitative_summarized\u00a0, is defined\u00a0below:</p>\n<a href=\"https://medium.com/media/f88403b030b8bcf497eb230c535c7681/href\">https://medium.com/media/f88403b030b8bcf497eb230c535c7681/href</a><p>Similar to categorical_summarized\u00a0, quantitative_summarized takes in a data frame and some input arguments to output the latent statistics and also a box plot and swarm plot (if swarm was set to true\u00a0).</p>\n<p>quantitative_summarized can take in one Quantitative Variable and up to two Categorical Variables, where the Quantitative Variable has to be assigned to y and the other two Categorical Variables can be assigned to x and hue respectively. Some example codes to show the usage is shown\u00a0below:</p>\n<h4>Some examples using `quantitative_summarized`</h4>\n<h4>Univariate Analysis</h4>\n<a href=\"https://medium.com/media/87d4dda79f594dd342ca4197f543f35f/href\">https://medium.com/media/87d4dda79f594dd342ca4197f543f35f/href</a><p>Would give the following:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/391/1*pReaeQwJUxasknHo531eug.png\"><figcaption>Output of quantitative_summarized on Age\u00a0Variable</figcaption></figure><h4>Bivariate Analysis</h4>\n<a href=\"https://medium.com/media/6809a642c394ab64383008d363a17c47/href\">https://medium.com/media/6809a642c394ab64383008d363a17c47/href</a><p>Would give the following:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/391/1*LNqRPGm70lPlRqc8o1iKzg.png\"><figcaption>Output of quantitative_summarized on Age Variable with x set to\u00a0Survived</figcaption></figure><h4>Multivariate Analysis</h4>\n<a href=\"https://medium.com/media/0ae25be5d77cffeee905492f98a068a7/href\">https://medium.com/media/0ae25be5d77cffeee905492f98a068a7/href</a><p>Would give the following:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/392/1*PIUUZAXGreqoX6m__Om0Ug.png\"><figcaption>Output of quantitative_summarized on Age Variable with x set to Survived and hue set to\u00a0Pclass</figcaption></figure><p><strong>Correlation Analysis</strong></p>\n<p>Another popular quantitative analysis we could use is to find the correlation between our variables. Correlation is a way to understand the relationship between the variables in our dataset. For any pairs of variables we\u00a0have:</p>\n<ul>\n<li>\n<strong>Positive Correlation</strong>: both variables change in the same direction</li>\n<li>\n<strong>Neutral Correlation</strong>: No relationship in the change of the variables</li>\n<li>\n<strong>Negative Correlation</strong>: variables change in opposite directions</li>\n</ul>\n<p>The Pearson\u2019s correlation coefficient is commonly used to summarize the strength of the linear relationship between two sample variables. It is the normalization of the covariance between the two variables which is given\u00a0as,</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/238/0*VaKgak2trkYdMz-w\"><figcaption>Pearson\u2019s Correlation Coefficient Formula</figcaption></figure><p>A good way to visualize this is with a heatmap. We first drop the categorical variables and fill up the missing values of the remaining quantitative variables with their mode. Pandas DataFrames allow for an easy calculation of Pearson\u2019s CC with a\u00a0.corr method. The code can be found\u00a0below</p>\n<a href=\"https://medium.com/media/1c5f489cf19b37477aa8dedc06ce0407/href\">https://medium.com/media/1c5f489cf19b37477aa8dedc06ce0407/href</a><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/759/1*rQEQlyiHpQ2nam7IhKBzsQ.png\"><figcaption>Heatmap of Pearson\u2019s CC for Quantitative Variables</figcaption></figure><p>One thing useful about having this visualization is that we can not only see the relationships between variables, for example, there is a negative Pearson\u2019s CC between Fare and Pclass, we could avoid the phenomenon of <strong>multicollinearity</strong> from happening.</p>\n<p>Multicollinearity can adversely affect generalized linear models, such as logistic regression. It occurs when there are high correlations between variables, resulting in unstable estimates of regression coefficients. However, if you are planning to use a Decision Tree for your predictor, then you need not need to worry about\u00a0this.</p>\n<p>An interesting read about collinearity can be found\u00a0<a href=\"https://medium.com/future-vision/collinearity-what-it-means-why-its-bad-and-how-does-it-affect-other-models-94e1db984168\">here</a>.</p>\n<p>Correlation can help with data imputation, where correlated variables can be used to fill in the miss values of each other. Correlation can also indicate the presence of a causal relationship. With that being said, it is important to note that <a href=\"https://en.wikipedia.org/wiki/Correlation_does_not_imply_causation\">correlation does not imply causation</a>.</p>\n<p>For more examples, please refer to the Titanic.ipynb in the GitHub\u00a0repo.</p>\n<p><a href=\"https://medium.com/@renjietan?source=rss-fdf264797c2a------2#4603\"><em>Back to\u00a0outline</em></a></p>\n<h3>4. Clustering</h3>\n<h4>k-Means Clustering</h4>\n<p>k-means clustering belongs to the family of <strong>partitioning clustering</strong>. In Partition clustering, we have to specify the number of clusters, <em>k</em>, that we want. This can be done by picking out the \u201celbow\u201d point of an inertia plot shown below. <strong>A good choice of <em>k</em>, is one that is not too large and has a low inertia value</strong>. The inertia is a metric to measure the cluster quality by giving us an indicator of how tightly packed a cluster\u00a0is.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/392/1*RIQ2JkabVK07E9LrE675lg.png\"><figcaption>Inertia plot using K Means Clustering to select a suitable k\u00a0value</figcaption></figure><p>As K Means calculates the distance between the features to decide if the following observation belongs to a certain centroid, we have to preprocess our data by encoding our Categorical Variables and filling up missing values. A simple function to preprocess is shown\u00a0below.</p>\n<a href=\"https://medium.com/media/5b38fcb30ca4e9716676a8780d396482/href\">https://medium.com/media/5b38fcb30ca4e9716676a8780d396482/href</a><p>Now that we have treated our data, we have to perform <strong>feature scaling</strong> so that the distance calculated across the features can be compared. This is done easily via sklearn.preprocessing library. For code implementation, please refer to the Titanic.ipynb\u00a0. After running the k-means algorithm, where we set <em>k = 2</em>, we can plot the variables as shown\u00a0below.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/497/1*GN4o-UBZxatBILXZQYto8g.png\"><figcaption>Cluster red and blue plotted on Age and Survived\u00a0Axes</figcaption></figure><h4>Hierarchical Agglomerative Clustering</h4>\n<p>For this subsection, I would present another quick way to perform EDA via clustering. Agglomerative clustering uses a bottom-up approach where individual observations are joined together iteratively based on their distance. We would be using the scipy.cluster.hierarchy package to perform the linkages and show our results using a dendrogram. The distance between two clusters is calculated via the <strong>nearest neighbor\u00a0method</strong>.</p>\n<a href=\"https://medium.com/media/0b539f45e4566dd240147d3c91447c28/href\">https://medium.com/media/0b539f45e4566dd240147d3c91447c28/href</a><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/933/1*NYpmiX4PVC75OyFEUTabFg.png\"><figcaption>Dendrogram of the Hierarchical Clustering of the Titanic\u00a0Dataset</figcaption></figure><p><a href=\"https://medium.com/@renjietan?source=rss-fdf264797c2a------2#4603\"><em>Back to\u00a0outline</em></a></p>\n<h3>5. Feature Importance by Tree-based estimators</h3>\n<p>Another quick way to perform EDA is via tree-based estimators. A decision tree learns how to \u201cbest\u201d split the dataset into smaller subsets before finally outputting the predicted leaf node. The split is commonly defined by an <strong>impurity criterion</strong> like <strong>Gini</strong> or <strong>Information Gain Entropy</strong>. Since this is a post on EDA and not decision trees, I shall not explain the maths behind them in detail, but I shall show you how you could use them to understand your features better. You may refer to this <a href=\"https://medium.com/@srnghn/the-mathematics-of-decision-trees-random-forest-and-feature-importance-in-scikit-learn-and-spark-f2861df67e3\">well-written article</a> for more\u00a0details.</p>\n<p>Based on the impurity criterion, a tree can be built by greedily picking the features that contribute to the most information gain. To illustrate this, I shall use the scikit-learn library.</p>\n<h4>Build a Random Forest Classifier</h4>\n<p>We first build a random forest classifier. By default, the impurity criterion is set to Gini. Using the following code, we could see the corresponding feature importance of our Titanic\u00a0dataset.</p>\n<a href=\"https://medium.com/media/4728ebe8147ab73ec45d506322b006e0/href\">https://medium.com/media/4728ebe8147ab73ec45d506322b006e0/href</a><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/633/1*W98z_WNBSBa5fpzSlnMtJA.png\"></figure><h4>Let\u2019s try\u00a0XGBoost</h4>\n<p>Another way to create an ensemble of decision trees is via XGBoost, which is part of the family of gradient boosted framework. Using the following code, we could see which corresponding feature is important to our XGBoost. Similarly, by default, the impurity criterion is set to\u00a0Gini.</p>\n<a href=\"https://medium.com/media/97ad96a6056c9d2b76fa0f0f2be99ec1/href\">https://medium.com/media/97ad96a6056c9d2b76fa0f0f2be99ec1/href</a><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/633/1*ANVyGW9NW6I7VeahNjT1JA.png\"></figure><p><a href=\"https://medium.com/@renjietan?source=rss-fdf264797c2a------2#4603\"><em>Back to\u00a0outline</em></a></p>\n<h3>6. Dashboarding Techniques</h3>\n<p>Unlike infographics, dashboards are created to objectively display important information in a clean, concise manner on a single screen with the purpose to inform and not misguide its readers. Very often, dashboards are a representation of the data that exploit our visual perception abilities in order to amplify cognition. The information they display is of high graphical excellence and is understood by all, requiring no supplementary information for interpretation.</p>\n<p>To achieve Graphical Excellence, the following two key aspects have to be\u00a0obeyed:</p>\n<ol>\n<li>Maximize Data: Ink and Minimize Chartjunk</li>\n<li>Have High Graphical Integrity</li>\n</ol>\n<h4>Data: Ink &amp; Chartjunk</h4>\n<p>Data: Ink is defined as the ink used to represent data, while chartjunk is defined as the superfluous, decorative, or diverting ink. Examples of chartjunk are moir\u00e9 vibration, grids, and the duck. To achieve this, we make use of our <strong>pre-attentive attributes</strong> and <strong>Gestalt's Principles</strong> to bring out patterns in visualizations.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/443/1*pwJ9guekqUn1ItbxTiXu0w.png\"><figcaption>Image showing the side of a man\u2019s face, at the same time, the front of his face. An example to illustrate Figure &amp;\u00a0Ground</figcaption></figure><h4>Achieving Graphical Integrity</h4>\n<p>Adapted from: Tufte, Edward. (2001). <a href=\"https://www.amazon.com/Visual-Display-Quantitative-Information/dp/1930824130\">The Visual Display of Quantitative Information, 2nd Editions, Graphics</a>, there are six principles to ensure Graphical Integrity:</p>\n<ol>\n<li>Make the representation of numbers proportional to quantities</li>\n<li>Use clear, detailed, and thorough\u00a0labeling</li>\n<li>Show data variation, not design variation</li>\n<li>Use standardized units, not nominal\u00a0values</li>\n<li>Depict \u2019n\u2019 data dimensions with less than or equal to \u2019n\u2019 variable dimensions</li>\n<li>Quote data in full\u00a0context</li>\n</ol>\n<p>As such, we avoid things like pie charts or putzing around with 3D graphs or with area dimensions. Bar graphs and box plots are such good examples to achieve graphical integrity as they are simple (everyone could understand without ambiguity) and powerful. It is also important to not miss out on context, like having the axis displaying quantitative data referenced to\u00a0zero.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*JMGUtWECuUve-cMtv4CbCg.png\"><figcaption>Examples of Misleading Graphics with Poor Graphical Integrity</figcaption></figure><p>Till date, <a href=\"https://www.tableau.com/\">Tableau</a> is probably the industry leader when it comes to dashboarding. They take in best dashboarding practices and also do the heavy lifting of plotting the graphs for you just by dragging and dropping. I would highly recommend anyone who has an interest in this field to take a look at Tableau. Another up and coming open-source project, which does similar things to Tableau, is called <a href=\"https://superset.incubator.apache.org/\">Apache Superset</a>.</p>\n<p><a href=\"https://medium.com/@renjietan?source=rss-fdf264797c2a------2#4603\"><em>Back to\u00a0outline</em></a></p>\n<h3>Conclusion</h3>\n<p>All in all, there are numerous ways to do EDA. EDA is part of the entire data science process, which is highly iterative. What I have shared in this article are things which I have picked up and stayed with me after joining a number of data science hackathons, my time as a data scientist in <a href=\"https://www.aisingapore.org/\">AI Singapore</a>, as well as my projects in my <a href=\"https://www.iss.nus.edu.sg/graduate-programmes/programme/detail/master-of-technology-in-knowledge-engineering\">MTech course</a>. I firmly believe in first establishing a firm foundation and then we optimize for speed and performance and hence, my motivation for writing this post. With this set of helper functions, my team and I were able to understand the latent statistics of our given dataset, and then develop a sound strategy to build a classifier within half a day of\u00a0hacking.</p>\n<p>This post is not meant to be a complete guide to EDA, but a Starter Pack to get aspiring Data Scientist kickoff their first data science projects using open-source tools like Python, pandas, seaborn, and scikit-learn. If you have any cool custom functions that are built upon Python packages and have aided you in your EDA process, please feel free to leave a comment below with your code. You could also submit a pull request and contribute to the helper.py file. Thanks for reading \ud83d\ude04. I hope you took away a thing or\u00a0two.</p>\n<p><em>Special mention to </em><a href=\"https://towardsdatascience.com/@raimibinkarim\"><em>Raimi</em></a><em>, </em><a href=\"https://towardsdatascience.com/@derekchia\"><em>Derek</em></a><em>, and </em><a href=\"https://medium.com/@cykoay04157\"><em>Chin Yang</em></a><em> for proofreading and giving me feedback on improving this\u00a0article</em></p>\n<p><em>Feel free to connect with me via </em><a href=\"https://twitter.com/rahjaytee\"><em>Twitter</em></a><em>, </em><a href=\"https://www.linkedin.com/in/renjietan/\"><em>LinkedIn</em></a><em>!</em></p>\n<p><em>If you are interested in the other project that I have worked on, feel free to visit my\u00a0</em><a href=\"https://github.com/notha99y\"><em>GitHub</em></a><em>!</em></p>\n<p><em>Want to learn how to build a Human Action Classifier quickly? Do check out my previous post over\u00a0</em><a href=\"https://towardsdatascience.com/6-steps-to-quickly-train-a-human-action-classifier-with-validation-accuracy-of-over-80-655fcb8781c5\"><em>here</em></a><em>.</em></p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=a77889485baf\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://towardsdatascience.com/a-starter-pack-to-exploratory-data-analysis-with-python-pandas-seaborn-and-scikit-learn-a77889485baf\">A Starter Pack to Exploratory Data Analysis with Python, pandas, seaborn, and scikit-Learn</a> was originally published in <a href=\"https://towardsdatascience.com/\">Towards Data Science</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","enclosure":{},"categories":["scikit-learn","exploratory-data-analysis","data-science","python","pandas"]},{"title":"6 Steps to quickly train a Human Action Classifier with Validation Accuracy of over 80%","pubDate":"2018-12-01 13:53:57","link":"https://towardsdatascience.com/6-steps-to-quickly-train-a-human-action-classifier-with-validation-accuracy-of-over-80-655fcb8781c5?source=rss-fdf264797c2a------2","guid":"https://medium.com/p/655fcb8781c5","author":"Ren Jie Tan","thumbnail":"https://cdn-images-1.medium.com/max/1024/1*ihXSiiYNAKlTxcVFAg4b3w.jpeg","description":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*ihXSiiYNAKlTxcVFAg4b3w.jpeg\"><figcaption>Photo by <a href=\"https://pixabay.com/en/users/RichFL-644541/\">RichFL</a> from\u00a0<a href=\"https://pixabay.com/\">pixabay</a></figcaption></figure><h3>How I improved a Human Action Classifier to 80% Validation Accuracy in 6 Easy\u00a0Steps</h3>\n<p>How many of you are master procrastinators? \ud83d\ude4c If you are, you have come to the right\u00a0place.</p>\n<p>In this post, I would like to share with you guys some tips and tricks I have picked up during my time as a Data Scientist and how I used them to quickly beef up my model. I shall demonstrate this by adopting an ensemble approach of performing Human Action Classification on the University of Texas at Dallas Multimodal Human Action Dataset (<a href=\"https://www.utdallas.edu/~kehtar/UTD-MHAD.html\">UTD-MHAD</a>). The ensemble achieved a validation accuracy of 0.821 which is a significant improvement from the baseline paper\u2019s accuracy of\u00a00.672.</p>\n<h3>Background (The\u00a0Problem)</h3>\n<p>I was tasked to apply data fusion on UTD-MHAD to build a model to classify 27 different human actions, and like all master procrastinators, I left it to the last week to start doing it. *MAXIMUM PRESSURE = HIGHEST PRODUCTIVITY!*</p>\n<p>The UTD-MHAD is an open dataset collected from a Kinect camera and one wearable inertial sensor. The dataset contains 27 actions performed by 8 subjects (4 females and 4 males) with each subject repeating each action 4 times. After removing 3 corrupted sequences, the dataset is left with 861 data sequences. The dataset contains 4 data modalities, namely:</p>\n<ol>\n<li>RGB videos (spatio-temporal)</li>\n<li>Depth videos (spatio-temporal)</li>\n<li>Skeleton joint positions (spatio-temporal)</li>\n<li>inertial sensor signals (temporal)</li>\n</ol>\n<p>All 4 modalities were time synchronized and stored in\u00a0.avi and\u00a0.mat format respectively.</p>\n<h4>Task: Beat the baseline accuracy of\u00a00.672</h4>\n<p>The dataset came with a paper (<a href=\"https://www.researchgate.net/publication/279191574_UTD-MHAD_A_Multimodal_Dataset_for_Human_Action_Recognition_Utilizing_a_Depth_Camera_and_a_Wearable_Inertial_Sensor\">C.Chen, 2015</a>) which uses a Collaborative Representation Classifier (CRC) that had a validation accuracy of 0.672. This was calculated on a train-validation split where subjects 1, 3, 5, 7 were used for training, and subjects 2, 4, 6, 8 for validation and it was also the baseline accuracy I have to\u00a0beat!</p>\n<p>All fired up, I immediately went on to the web to start looking for past codes and tutorials. After spending roughly 30 mins on the web, I soon came to realize that there is no re-useable code! *STRESS LEVEL INCREASE*. It then dawned upon me that I had to start doing this all from scratch. I quickly took out a pen and notebook and started devising my strategy.</p>\n<h3>Code</h3>\n<p><a href=\"https://github.com/notha99y/Multimodal_human_actions\">https://github.com/notha99y/Multimodal_human_actions</a></p>\n<h3>Overview of the 6\u00a0steps</h3>\n<ol>\n<li><a href=\"https://medium.com/@renjietan?source=rss-fdf264797c2a------2#01cb\">Understand the\u00a0data</a></li>\n<li><a href=\"https://medium.com/@renjietan?source=rss-fdf264797c2a------2#8d80\">Quickly prototype</a></li>\n<li><a href=\"https://medium.com/@renjietan?source=rss-fdf264797c2a------2#4de3\">Performance metrics</a></li>\n<li><a href=\"https://medium.com/@renjietan?source=rss-fdf264797c2a------2#a17d\">Automate the parts you can, and ship your training to Google\u00a0Colab</a></li>\n<li><a href=\"https://medium.com/@renjietan?source=rss-fdf264797c2a------2#fc19\">Google the web and discuss with colleagues to get inspiration</a></li>\n<li><a href=\"https://medium.com/@renjietan?source=rss-fdf264797c2a------2#176c\">Ensemble your\u00a0models</a></li>\n</ol>\n<h3>Step 1: Understand the\u00a0data</h3>\n<p>Before you begin anything, it is important to know what you are dealing with. In this case, the best way is to plot it! I used NumPy, SciPy, and Matplotlib libraries to efficiently achieve these. Below are the plots of the Depth, Skeleton, and Inertial data of a subject performing a tennis\u00a0swing.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/333/1*GQ_5JoPkGMIZPgVtenhcQQ.png\"><figcaption>Video screenshot of a Tennis\u00a0Swing</figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/432/1*AHvQvfZ1sXF16iEPXFqvuQ.gif\"><figcaption>Depth videos of a Tennis\u00a0Swing</figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/640/1*XAShT_C9EVVPz0OAeR-Qig.gif\"><figcaption>Skeleton joint positions of a Tennis\u00a0Swing</figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1008/1*iuZ7lJS6Za5ajVclCr60XA.png\"><figcaption>Inertial sensor signals of a Tennis\u00a0Swing</figcaption></figure><p>So now that we have plotted them, we have to convert them to a suitable format to feed our model. My choice is NumPy array. For this post, I would focus mainly on just using the Skeleton and Inertial Data. For the RGB and Depth videos, special care in creating a VideoDataGenerator is required to read them from disk as they are too big to load on\u00a0memory.</p>\n<p>The Skeleton and Inertial Data have varying periods and for the Inertial sensors, varying amplitudes. Histogram plotting is an effective way to show the distribution of\u00a0these.</p>\n<h4>Period Distribution</h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/432/1*bFyHXfHLVIRa6zrWml8_vA.png\"><figcaption>Period Distribution of Inertial sensor\u00a0data</figcaption></figure><p>This should not come as a surprise as these are various actions performed by different subjects. The experiment also did not specify how a particular action should be carried out, so I am guessing the subject would just execute the action based on their own experience.</p>\n<p>Having these varying periods simply would not fly as our models would require a fixed input shape. We have two strategies to treat\u00a0this:</p>\n<ol>\n<li>zero-pad the signals all to the max length of\u00a0326</li>\n<li>resample the signals to a mean period of\u00a0180</li>\n</ol>\n<h4>Amplitude Distribution</h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/432/1*4SepMz4MgRhu0SgxatQcPg.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/432/1*UH_iqwNXLkh-Gj73QPhhfA.png\"><figcaption>Amplitude Distribution of 3-axial Gyroscope data (min on the left, max on the\u00a0right)</figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/432/1*XmhKLNK08t7jyt5LDHFvzQ.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/432/1*Tt_SeUHjBdahxmUHIX6Z6Q.png\"><figcaption>Amplitude Distribution of 3-axial Accelerometer data (min on the left, max on the\u00a0right)</figcaption></figure><p>The distribution of the amplitude resembles greatly to a long tail. Since the amplitude does not affect the shape of our input data, we could choose not to apply any pre-processing techniques on it. Otherwise, normalization techniques such as the mean-variance normalization could be applied for pre-processing.</p>\n<p><a href=\"https://medium.com/@renjietan?source=rss-fdf264797c2a------2#7343\"><em>Back to\u00a0overview</em></a></p>\n<h3>Step 2: Quickly prototype</h3>\n<p>As the Lean Startup approach preaches, \u201cFail Fast, Fail Cheap\u201d. The next step is to build a light-weight model that allows for quick iteration. <a href=\"https://keras.io/\">Keras</a>, the high-level neural network wrapper written in Python, would be the framework of choice for this task. Keras allows a clean, minimalist approach for you to build huge deep learning models with just a few lines of code. You could see how easy it is in the code implementation in the repo. P.S. We would also be using it with a Tensorflow backend.</p>\n<p>We shall first begin by only using the Inertial data. Since the data is a sequence of 6 channels (3-axis for accelerometer + 3-axis for gyroscope), the very first model we would be building is a <strong>Simple LSTM</strong> (<a href=\"https://www.mitpressjournals.org/doi/10.1162/neco.1997.9.8.1735\">S. Hochreiter et al., 1997</a>) with a LSTM cell of 512 hidden\u00a0units.</p>\n<a href=\"https://medium.com/media/94bc53b72a964952d393052fa0bba209/href\">https://medium.com/media/94bc53b72a964952d393052fa0bba209/href</a><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/246/1*252t4HDPRw7h_28oQXbzrA.png\"><figcaption>Network Diagram of Simple LSTM\u00a0model</figcaption></figure><p><a href=\"https://medium.com/@renjietan?source=rss-fdf264797c2a------2#7343\"><em>Back to\u00a0overview</em></a></p>\n<h3>Step 3. Performance metrics</h3>\n<p>With the model created, we now need a reliable feedback system to inform us on how the model is performing. As this a classification task with a well-balanced class distribution, the accuracy would suffice as the sole performance metric, without the need of calculating precision, recall or F1-score.</p>\n<p>To see if our model is over-fitting, we can also get Train-Validation Accuracy-Loss plot. A 27-Class confusion matrix can also be plotted to see which are the actions that are often misclassified as\u00a0another.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/481/1*ZQ0Zh_UFMd9biTrzDVMbKg.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/481/1*h23jlTtAahlRTcWYJpVXQg.png\"><figcaption>Loss (Left) Accuracy (Right) plots of the Training (Blue) and Validation (Green) set of the Simple\u00a0LSTM</figcaption></figure><p>From the Accuracy-Loss plots, we can see that our model is over-fitting at very early epochs, with our validation accuracy plateauing after the 4th epoch. At epoch 15, we got a model with a validation accuracy of ~ 0.238, which is pretty far off from the baseline of 0.672 we have to\u00a0beat.</p>\n<p>This suggests that we would have to either change strategy or apply more regularization techniques such as Dropout\u00a0layers.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/864/1*gt8tOUz2YhMLkg6TgIQQZQ.png\"><figcaption>Confusion matrix of the Simple LSTM model on Inertial\u00a0data</figcaption></figure><p>Oh gosh! This confusion matrix looks like a screenshot of a minesweeper game! The only saving graces were the \u201cstand to sit\u201d and \u201csit to stand\u201d actions which the model predicted 16 (perfect score) and 13 correctly, respectively. The other 25 actions had very poor performance.</p>\n<p>Before we go off stressing ourselves out, let us take a step back and look what we have done thus\u00a0far.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/673/1*oB2bO2-ypVUZ9mlhoop9rw.png\"><figcaption>Data Science Pipeline (adapted from Practical Data Science Cook\u00a0Book)</figcaption></figure><p>We have just finished one full iteration of going from Step 1 -&gt; 4 in the above flowchart and we got a first validation accuracy of 0.238. This is nowhere ideal, but it is a pretty good first start. We have set ourselves up with a highly iterative Data Science Pipeline where we could efficiency explore, build, and evaluate our project. Ask any practitioner and they would all agree that Data Science is a highly iterative journey.</p>\n<p>With this foundation formed, we can now get creative and try different stuff to improve our model. I shall spare you guys the agony of seeing all the different trials I tried, so in the following sections, I shall just show you all the key results I found using this iterative pipeline.</p>\n<h4>Pre-processing</h4>\n<p>With this pipeline, I also found that re-sampling the sequences to the mean of 180 leads to better convergence compared to zero padding. Normalization of the amplitude led to no obvious improvement of the model performance, so we would skip it to prevent unnecessary calculation.</p>\n<p><a href=\"https://medium.com/@renjietan?source=rss-fdf264797c2a------2#7343\"><em>Back to\u00a0overview</em></a></p>\n<h3>Step 4. Automate the parts you can, and ship your training to Google\u00a0Colab</h3>\n<p>Since we would most probably be repeating certain steps quite\u00a0often, it is worthwhile to take some time and automate them. We can convert certain frequently used code into scripts and perform functional abstraction on them. Your not-so-future self would be highly grateful for you doing\u00a0this.</p>\n<h4>Keras Callbacks</h4>\n<p>The Keras callbacks are one of the best things that can happen to anyone who is trying to dabble their feet into deep learning. They are tools which would automate your model training and I shall share 3 of my favorite callbacks which greatly aid me in my various projects.</p>\n<p>First, the <strong>TensorBoard</strong>. This allows Keras to save an event log file which constantly updates during the training and can be read and viewed by TensorBoard. This allows for a real-time, graphical visualization of your model training and I highly recommend it as an alternative then just viewing it from Keras\u2019s model.fit() output.</p>\n<p>Second, the <strong>ModelCheckpoint</strong>. This allows your Keras model to save weights to a given file directory. There are useful arguments such as monitor and save_best_only which give you some control over how you want Keras to save your\u00a0weights.</p>\n<p>Last but not the least, the <strong>EarlyStopping</strong> callback. Having this would allow Keras to stop your training based on the condition you specify. For my case, as shown below, I set min_delta=0 and patience=5. This means that Keras would stop the training if it finds that the model\u2019s validation accuracy is not increasing after 5\u00a0epochs.</p>\n<p>With these 3 callbacks set in place, we can safely leave our model training while we head out for\u00a0lunch.</p>\n<a href=\"https://medium.com/media/3cdfcf3393ac1c23e706c44871384684/href\">https://medium.com/media/3cdfcf3393ac1c23e706c44871384684/href</a><h4>Google Colaboratory</h4>\n<p>As we all know, training Deep Learning models is a very GPU intensive process. Luckily for us, <a href=\"https://colab.research.google.com/\">Google Colaboratory</a> has provided powerful TPU kernels for free! For those who cannot afford a powerful GPU can consider shipping your training to Google Colab. Google Colab also provides a familiar Jupyter notebook-like interface, making it very intuitive to use. It is also mounted on your Google Drive, so you can easily read your data into Colab. Weights and logs can also be easily\u00a0saved.</p>\n<p><a href=\"https://medium.com/@renjietan?source=rss-fdf264797c2a------2#7343\"><em>Back to\u00a0overview</em></a></p>\n<h3>Step 5. Google the web and discuss with colleagues to get inspiration</h3>\n<p>With a semi-automated pipeline of fast prototyping and evaluation done in sections 2\u20134, it is time to get inspiration and find creative ways to improve our model\u2019s validation accuracy. Google different search terms, or going to portals like Google Scholar, Science Direct and Pubmed could give us insights. Chatting with colleagues about your problem could give us serendipitous, \u201cEureka\u201d\u00a0moments.</p>\n<p>I was chatting with a colleague who was working on a Natural Language Processing (NLP) project that gave me the inspiration to try a Bi-Directional LSTM (BLSTM) (<a href=\"https://ieeexplore.ieee.org/document/650093\">M. Schuster et al., 1997</a>). The BLSTM reverses the original hidden layers and connects them, allowing a form of generative deep learning, resulting in the output layer getting both information from the past and future states simultaneously. Just by adding a layer of BLSTM, doubled my validation accuracy to\u00a00.465.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/506/1*EagmbkrKX-x98L9Qu7-_zw.png\"><figcaption>Network Diagram of Bi-Directional LSTM\u00a0model</figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/246/1*k33UYW3fqAfmiq5qQZTtlA.png\"><figcaption>Network Diagram of Conv LSTM\u00a0model</figcaption></figure><h4>Conv LSTM\u00a0model</h4>\n<p>The main breakthrough came when I added Convolutional layers for feature extraction. As the input data is a 1D Signal, this model uses a series of 1D Convolutional and 1D Maxpooling layers to extract higher dimensional, latent features before feeding them into 2 LSTM units which capture the temporal information. The output of the LSTM units is then flattened out and we attached a Dropout layer with a dropout rate of 0.5 before adding a Dense layer with a softmax activation to classify all 27\u00a0actions.</p>\n<p>This has got my validation accuracy to 0.700 just on the Inertial data which is the first time we beat the CRC model baseline of 0. For all our models, we used the AdamOptimizer (<a href=\"https://arxiv.org/abs/1412.6980\">D. P. Kingma et al.m 2014</a>) with a<br>learning rate of 1e\u22124, \u03b21 of 0.9, and \u03b22 of 0.999. We initialize our trainable parameters using the Xavier Glorot initializer (<a href=\"http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf\">X. Glorot et at.\u00a0,2010</a>) and set our batch size to 3 to allow our model our model to generalize better (<a href=\"https://arxiv.org/abs/1705.08741\">E. Hoffer et al.,\u00a02017</a>).</p>\n<h4>UNet LSTM\u00a0model</h4>\n<p>The UNet (<a href=\"https://arxiv.org/abs/1505.04597\">O. Ronneberger et al., 2015</a>) is a Fully Convolutional Neural Network (FCNN) that is almost symmetric in the contraction and expansion path. In the contraction path, the input was is being fed through a series of convolutions and max-pooling, increasing the feature maps and decreasing the resolution of the image. This increases the \u201cwhat\u201d and decreases the \u201cwhere\u201d. In the expansion path, the high dimensional features with low resolution are being up-sampled via convolutional kernels. The features maps were reduced during this operation. A novel feature of UNet is that it implements a concatenation of high dimensional features in the contraction path to the low dimensional feature maps of the expansion layers. Similarly, I added the extracted features from the convolutional networks into 2 LSTM units, flattened the output and attached a Dropout layer with a dropout rate of 0.5 finishing off with a Dense layer with a softmax activation to classify all 27 actions. I have attached the Network Diagram in the Appendix\u00a0below.</p>\n<p>The UNet LSTM model achieved a validation accuracy of 0.712 on the Inertial\u00a0data.</p>\n<p><a href=\"https://medium.com/@renjietan?source=rss-fdf264797c2a------2#7343\"><em>Back to\u00a0overview</em></a></p>\n<h3>Step 6. Ensemble your\u00a0models</h3>\n<p>With both Conv LSTM and UNet LSTM performing pretty well on the validation data, we can combine their softmax outputs by taking the average. This immediately increases the validation accuracy to\u00a00.765!</p>\n<p>For most Supervised Learning problems, the ensemble method tends to outperform a single model method. This is currently understood to be because of its ability to transverse the hypothesis space. An ensemble is able to derive a better hypothesis that is not in the hypothesis space of its single models from which it is\u00a0built.</p>\n<p>Empirically, ensembles tend to yield better results when there is diversity among the model (<a href=\"http://machine-learning.martinsewell.com/ensembles/KunchevaWhitaker2003.pdf\">L. Kuncheva et al., 2003</a>). From the Confusion Matrices shown below, we can see that the Conv LSTM is able to pick up actions like swipe to the right and squat better, while the UNet LSTM is able to pick up actions like basketball shoot and draw x better. This indicates that there is model diversity among the two models and true enough, by ensembling them together, we got the validation accuracies from 0.700 and 0.712 to\u00a00.765!</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/766/1*LWo0ebGq4CSs1aX2Aoo4Og.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/767/1*p99IzsqqGfGPtzxWRH2Chw.png\"><figcaption>Confusion Matrices of Conv LSTM (left) and UNet LSTM (right) on Inertial\u00a0data</figcaption></figure><p>Below is the equation I used to create the ensemble. For code implementation, please refer to the\u00a0repo.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/486/1*U36T1fgWG7Y3DbkZegssEw.png\"><figcaption>Average of the softmax output for the Conv LSTM and UNet\u00a0LSTM</figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/366/1*PyoVd_UXzyJB-PIg8XO13g.png\"><figcaption>Softmax output for an action\u00a0j</figcaption></figure><h4>Combining with the Skeleton\u00a0data</h4>\n<p>To achieve the promised 80% validation accuracy as stated in the title, I added the Skeleton data by also resampling it to a period of 180 units. After fusing this with the 6 channel Inertial data, we have an input shape of (N, 180, 66), where N is the number of samples. A table of all the validation accuracies is compiled\u00a0below.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/502/1*4Zhs9aGEm9yYVswxod8TYw.png\"><figcaption>Summary of Validation Accuracy of the Different Models</figcaption></figure><p>Lo and behold, the confusion matrix of our best performing model with a validation accuracy of 0.821 is shown\u00a0below.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/772/1*E1AqJQmUImSrU__g0z3iRw.png\"><figcaption>Confusion Matrix of Ensemble on Inertial + Skeleton\u00a0data</figcaption></figure><p><a href=\"https://medium.com/@renjietan?source=rss-fdf264797c2a------2#7343\"><em>Back to\u00a0overview</em></a></p>\n<h3>Summary</h3>\n<p>Congratulations on making it all the way here! If you have followed these steps thoroughly, you would have successfully built your very own ensembled Human Action Classifier!</p>\n<h4>Model zoo</h4>\n<ul>\n<li><a href=\"https://medium.com/@renjietan?source=rss-fdf264797c2a------2#bd18\">Simple LSTM</a></li>\n<li><a href=\"https://medium.com/@renjietan?source=rss-fdf264797c2a------2#8031\">Bi-Directional LSTM</a></li>\n<li><a href=\"https://medium.com/@renjietan?source=rss-fdf264797c2a------2#1aad\">Conv LSTM</a></li>\n<li><a href=\"https://medium.com/@renjietan?source=rss-fdf264797c2a------2#5fc1\">UNet LSTM</a></li>\n</ul>\n<h4>Some key takeaways</h4>\n<ul>\n<li>Plotting is a quick and easy way to understand your\u00a0data</li>\n<li>Data Science is a highly iterative process</li>\n<li>Automate the things you\u00a0can</li>\n<li>Ensemble is a quick way to get the best bang for your buck of our trained\u00a0models</li>\n<li>Use Google Colab to increase your training\u00a0speed</li>\n<li>Keras is the framework of choice for quick prototyping of deep learning\u00a0models</li>\n</ul>\n<p>If you are up for a challenge and feel that 0.821 is not enough, you may read the following subsection to improve your\u00a0model.</p>\n<h4>What more could be\u00a0done</h4>\n<h4>A. Issue of over-fitting</h4>\n<p>Throughout our training, over-fitting at early epochs seems to be the main recurring challenge that we faced. We tried adding Dropout layers and ensembling to make our model more generalized but we can still go further. Over-fitting tends to happen when our model tries to learn high-frequency features that may not be useful. Adding Gaussian Noise with zero mean and data points in all frequencies might enhance the learning capability of our<br>model. Similarly, the time sequences of different subjects are quite varied even for the same activities. Performing data augmentation using time scaling and translation would increase the amount of training data, allowing our model to generalize better.</p>\n<p>On a side note, our model could also be trimmed further to reduce its complexity, and also its risk of over-fitting. With the recent Neural Architecture Search papers like NAS (<a href=\"https://arxiv.org/abs/1611.01578\">B. Zoph et.al, 2016</a>), NASnet (<a href=\"https://arxiv.org/abs/1707.07012\">B.Zpoh et.al, 2017</a>) and Efficient-NAS (<a href=\"https://arxiv.org/abs/1802.03268\">H. Pham et.al, 2018</a>), gaining traction, we could also try applying them since this is also a classification task.</p>\n<h4>B. Data Fusion of RGB and Depth\u00a0Data</h4>\n<p>We played with the Inertial, and we added the Skeleton towards the end to get us more information to find our data-hungry models. In order to push our model more, we would have to find ways to fusion it with the Depth and RGB data. This would allow for more input training variables to learn and extract features from, hence improving the validation accuracies.</p>\n<h4>C. Try other Ensemble Learning Techniques</h4>\n<p>Instead of doing a simple average, we could try more advanced ensemble learning approaches such as Boosting and\u00a0Bagging.</p>\n<p><em>Special thanks to </em><a href=\"https://towardsdatascience.com/@raimibinkarim\"><em>Raimi</em></a><em> and </em><a href=\"https://medium.com/@derekchia\"><em>Derek</em></a><em> for proofreading and giving me feedback on this\u00a0article.</em></p>\n<p><em>Feel free to connect with me via </em><a href=\"https://twitter.com/rahjaytee\"><em>Twitter</em></a><em>, </em><a href=\"https://www.linkedin.com/in/renjietan/\"><em>LinkedIn</em></a><em>!</em></p>\n<p><em>If you are interested in other projects that I have worked on, feel free to visit my\u00a0</em><a href=\"https://github.com/notha99y\"><em>Github</em></a><em>!</em></p>\n<h3>Appendix</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/460/1*pR8nx6BDrc7vavUzcY1Wfw.png\"><figcaption>Network Diagram of UNet LSTM\u00a0model</figcaption></figure><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=655fcb8781c5\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://towardsdatascience.com/6-steps-to-quickly-train-a-human-action-classifier-with-validation-accuracy-of-over-80-655fcb8781c5\">6 Steps to quickly train a Human Action Classifier with Validation Accuracy of over 80%</a> was originally published in <a href=\"https://towardsdatascience.com/\">Towards Data Science</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","content":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*ihXSiiYNAKlTxcVFAg4b3w.jpeg\"><figcaption>Photo by <a href=\"https://pixabay.com/en/users/RichFL-644541/\">RichFL</a> from\u00a0<a href=\"https://pixabay.com/\">pixabay</a></figcaption></figure><h3>How I improved a Human Action Classifier to 80% Validation Accuracy in 6 Easy\u00a0Steps</h3>\n<p>How many of you are master procrastinators? \ud83d\ude4c If you are, you have come to the right\u00a0place.</p>\n<p>In this post, I would like to share with you guys some tips and tricks I have picked up during my time as a Data Scientist and how I used them to quickly beef up my model. I shall demonstrate this by adopting an ensemble approach of performing Human Action Classification on the University of Texas at Dallas Multimodal Human Action Dataset (<a href=\"https://www.utdallas.edu/~kehtar/UTD-MHAD.html\">UTD-MHAD</a>). The ensemble achieved a validation accuracy of 0.821 which is a significant improvement from the baseline paper\u2019s accuracy of\u00a00.672.</p>\n<h3>Background (The\u00a0Problem)</h3>\n<p>I was tasked to apply data fusion on UTD-MHAD to build a model to classify 27 different human actions, and like all master procrastinators, I left it to the last week to start doing it. *MAXIMUM PRESSURE = HIGHEST PRODUCTIVITY!*</p>\n<p>The UTD-MHAD is an open dataset collected from a Kinect camera and one wearable inertial sensor. The dataset contains 27 actions performed by 8 subjects (4 females and 4 males) with each subject repeating each action 4 times. After removing 3 corrupted sequences, the dataset is left with 861 data sequences. The dataset contains 4 data modalities, namely:</p>\n<ol>\n<li>RGB videos (spatio-temporal)</li>\n<li>Depth videos (spatio-temporal)</li>\n<li>Skeleton joint positions (spatio-temporal)</li>\n<li>inertial sensor signals (temporal)</li>\n</ol>\n<p>All 4 modalities were time synchronized and stored in\u00a0.avi and\u00a0.mat format respectively.</p>\n<h4>Task: Beat the baseline accuracy of\u00a00.672</h4>\n<p>The dataset came with a paper (<a href=\"https://www.researchgate.net/publication/279191574_UTD-MHAD_A_Multimodal_Dataset_for_Human_Action_Recognition_Utilizing_a_Depth_Camera_and_a_Wearable_Inertial_Sensor\">C.Chen, 2015</a>) which uses a Collaborative Representation Classifier (CRC) that had a validation accuracy of 0.672. This was calculated on a train-validation split where subjects 1, 3, 5, 7 were used for training, and subjects 2, 4, 6, 8 for validation and it was also the baseline accuracy I have to\u00a0beat!</p>\n<p>All fired up, I immediately went on to the web to start looking for past codes and tutorials. After spending roughly 30 mins on the web, I soon came to realize that there is no re-useable code! *STRESS LEVEL INCREASE*. It then dawned upon me that I had to start doing this all from scratch. I quickly took out a pen and notebook and started devising my strategy.</p>\n<h3>Code</h3>\n<p><a href=\"https://github.com/notha99y/Multimodal_human_actions\">https://github.com/notha99y/Multimodal_human_actions</a></p>\n<h3>Overview of the 6\u00a0steps</h3>\n<ol>\n<li><a href=\"https://medium.com/@renjietan?source=rss-fdf264797c2a------2#01cb\">Understand the\u00a0data</a></li>\n<li><a href=\"https://medium.com/@renjietan?source=rss-fdf264797c2a------2#8d80\">Quickly prototype</a></li>\n<li><a href=\"https://medium.com/@renjietan?source=rss-fdf264797c2a------2#4de3\">Performance metrics</a></li>\n<li><a href=\"https://medium.com/@renjietan?source=rss-fdf264797c2a------2#a17d\">Automate the parts you can, and ship your training to Google\u00a0Colab</a></li>\n<li><a href=\"https://medium.com/@renjietan?source=rss-fdf264797c2a------2#fc19\">Google the web and discuss with colleagues to get inspiration</a></li>\n<li><a href=\"https://medium.com/@renjietan?source=rss-fdf264797c2a------2#176c\">Ensemble your\u00a0models</a></li>\n</ol>\n<h3>Step 1: Understand the\u00a0data</h3>\n<p>Before you begin anything, it is important to know what you are dealing with. In this case, the best way is to plot it! I used NumPy, SciPy, and Matplotlib libraries to efficiently achieve these. Below are the plots of the Depth, Skeleton, and Inertial data of a subject performing a tennis\u00a0swing.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/333/1*GQ_5JoPkGMIZPgVtenhcQQ.png\"><figcaption>Video screenshot of a Tennis\u00a0Swing</figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/432/1*AHvQvfZ1sXF16iEPXFqvuQ.gif\"><figcaption>Depth videos of a Tennis\u00a0Swing</figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/640/1*XAShT_C9EVVPz0OAeR-Qig.gif\"><figcaption>Skeleton joint positions of a Tennis\u00a0Swing</figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1008/1*iuZ7lJS6Za5ajVclCr60XA.png\"><figcaption>Inertial sensor signals of a Tennis\u00a0Swing</figcaption></figure><p>So now that we have plotted them, we have to convert them to a suitable format to feed our model. My choice is NumPy array. For this post, I would focus mainly on just using the Skeleton and Inertial Data. For the RGB and Depth videos, special care in creating a VideoDataGenerator is required to read them from disk as they are too big to load on\u00a0memory.</p>\n<p>The Skeleton and Inertial Data have varying periods and for the Inertial sensors, varying amplitudes. Histogram plotting is an effective way to show the distribution of\u00a0these.</p>\n<h4>Period Distribution</h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/432/1*bFyHXfHLVIRa6zrWml8_vA.png\"><figcaption>Period Distribution of Inertial sensor\u00a0data</figcaption></figure><p>This should not come as a surprise as these are various actions performed by different subjects. The experiment also did not specify how a particular action should be carried out, so I am guessing the subject would just execute the action based on their own experience.</p>\n<p>Having these varying periods simply would not fly as our models would require a fixed input shape. We have two strategies to treat\u00a0this:</p>\n<ol>\n<li>zero-pad the signals all to the max length of\u00a0326</li>\n<li>resample the signals to a mean period of\u00a0180</li>\n</ol>\n<h4>Amplitude Distribution</h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/432/1*4SepMz4MgRhu0SgxatQcPg.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/432/1*UH_iqwNXLkh-Gj73QPhhfA.png\"><figcaption>Amplitude Distribution of 3-axial Gyroscope data (min on the left, max on the\u00a0right)</figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/432/1*XmhKLNK08t7jyt5LDHFvzQ.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/432/1*Tt_SeUHjBdahxmUHIX6Z6Q.png\"><figcaption>Amplitude Distribution of 3-axial Accelerometer data (min on the left, max on the\u00a0right)</figcaption></figure><p>The distribution of the amplitude resembles greatly to a long tail. Since the amplitude does not affect the shape of our input data, we could choose not to apply any pre-processing techniques on it. Otherwise, normalization techniques such as the mean-variance normalization could be applied for pre-processing.</p>\n<p><a href=\"https://medium.com/@renjietan?source=rss-fdf264797c2a------2#7343\"><em>Back to\u00a0overview</em></a></p>\n<h3>Step 2: Quickly prototype</h3>\n<p>As the Lean Startup approach preaches, \u201cFail Fast, Fail Cheap\u201d. The next step is to build a light-weight model that allows for quick iteration. <a href=\"https://keras.io/\">Keras</a>, the high-level neural network wrapper written in Python, would be the framework of choice for this task. Keras allows a clean, minimalist approach for you to build huge deep learning models with just a few lines of code. You could see how easy it is in the code implementation in the repo. P.S. We would also be using it with a Tensorflow backend.</p>\n<p>We shall first begin by only using the Inertial data. Since the data is a sequence of 6 channels (3-axis for accelerometer + 3-axis for gyroscope), the very first model we would be building is a <strong>Simple LSTM</strong> (<a href=\"https://www.mitpressjournals.org/doi/10.1162/neco.1997.9.8.1735\">S. Hochreiter et al., 1997</a>) with a LSTM cell of 512 hidden\u00a0units.</p>\n<a href=\"https://medium.com/media/94bc53b72a964952d393052fa0bba209/href\">https://medium.com/media/94bc53b72a964952d393052fa0bba209/href</a><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/246/1*252t4HDPRw7h_28oQXbzrA.png\"><figcaption>Network Diagram of Simple LSTM\u00a0model</figcaption></figure><p><a href=\"https://medium.com/@renjietan?source=rss-fdf264797c2a------2#7343\"><em>Back to\u00a0overview</em></a></p>\n<h3>Step 3. Performance metrics</h3>\n<p>With the model created, we now need a reliable feedback system to inform us on how the model is performing. As this a classification task with a well-balanced class distribution, the accuracy would suffice as the sole performance metric, without the need of calculating precision, recall or F1-score.</p>\n<p>To see if our model is over-fitting, we can also get Train-Validation Accuracy-Loss plot. A 27-Class confusion matrix can also be plotted to see which are the actions that are often misclassified as\u00a0another.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/481/1*ZQ0Zh_UFMd9biTrzDVMbKg.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/481/1*h23jlTtAahlRTcWYJpVXQg.png\"><figcaption>Loss (Left) Accuracy (Right) plots of the Training (Blue) and Validation (Green) set of the Simple\u00a0LSTM</figcaption></figure><p>From the Accuracy-Loss plots, we can see that our model is over-fitting at very early epochs, with our validation accuracy plateauing after the 4th epoch. At epoch 15, we got a model with a validation accuracy of ~ 0.238, which is pretty far off from the baseline of 0.672 we have to\u00a0beat.</p>\n<p>This suggests that we would have to either change strategy or apply more regularization techniques such as Dropout\u00a0layers.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/864/1*gt8tOUz2YhMLkg6TgIQQZQ.png\"><figcaption>Confusion matrix of the Simple LSTM model on Inertial\u00a0data</figcaption></figure><p>Oh gosh! This confusion matrix looks like a screenshot of a minesweeper game! The only saving graces were the \u201cstand to sit\u201d and \u201csit to stand\u201d actions which the model predicted 16 (perfect score) and 13 correctly, respectively. The other 25 actions had very poor performance.</p>\n<p>Before we go off stressing ourselves out, let us take a step back and look what we have done thus\u00a0far.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/673/1*oB2bO2-ypVUZ9mlhoop9rw.png\"><figcaption>Data Science Pipeline (adapted from Practical Data Science Cook\u00a0Book)</figcaption></figure><p>We have just finished one full iteration of going from Step 1 -&gt; 4 in the above flowchart and we got a first validation accuracy of 0.238. This is nowhere ideal, but it is a pretty good first start. We have set ourselves up with a highly iterative Data Science Pipeline where we could efficiency explore, build, and evaluate our project. Ask any practitioner and they would all agree that Data Science is a highly iterative journey.</p>\n<p>With this foundation formed, we can now get creative and try different stuff to improve our model. I shall spare you guys the agony of seeing all the different trials I tried, so in the following sections, I shall just show you all the key results I found using this iterative pipeline.</p>\n<h4>Pre-processing</h4>\n<p>With this pipeline, I also found that re-sampling the sequences to the mean of 180 leads to better convergence compared to zero padding. Normalization of the amplitude led to no obvious improvement of the model performance, so we would skip it to prevent unnecessary calculation.</p>\n<p><a href=\"https://medium.com/@renjietan?source=rss-fdf264797c2a------2#7343\"><em>Back to\u00a0overview</em></a></p>\n<h3>Step 4. Automate the parts you can, and ship your training to Google\u00a0Colab</h3>\n<p>Since we would most probably be repeating certain steps quite\u00a0often, it is worthwhile to take some time and automate them. We can convert certain frequently used code into scripts and perform functional abstraction on them. Your not-so-future self would be highly grateful for you doing\u00a0this.</p>\n<h4>Keras Callbacks</h4>\n<p>The Keras callbacks are one of the best things that can happen to anyone who is trying to dabble their feet into deep learning. They are tools which would automate your model training and I shall share 3 of my favorite callbacks which greatly aid me in my various projects.</p>\n<p>First, the <strong>TensorBoard</strong>. This allows Keras to save an event log file which constantly updates during the training and can be read and viewed by TensorBoard. This allows for a real-time, graphical visualization of your model training and I highly recommend it as an alternative then just viewing it from Keras\u2019s model.fit() output.</p>\n<p>Second, the <strong>ModelCheckpoint</strong>. This allows your Keras model to save weights to a given file directory. There are useful arguments such as monitor and save_best_only which give you some control over how you want Keras to save your\u00a0weights.</p>\n<p>Last but not the least, the <strong>EarlyStopping</strong> callback. Having this would allow Keras to stop your training based on the condition you specify. For my case, as shown below, I set min_delta=0 and patience=5. This means that Keras would stop the training if it finds that the model\u2019s validation accuracy is not increasing after 5\u00a0epochs.</p>\n<p>With these 3 callbacks set in place, we can safely leave our model training while we head out for\u00a0lunch.</p>\n<a href=\"https://medium.com/media/3cdfcf3393ac1c23e706c44871384684/href\">https://medium.com/media/3cdfcf3393ac1c23e706c44871384684/href</a><h4>Google Colaboratory</h4>\n<p>As we all know, training Deep Learning models is a very GPU intensive process. Luckily for us, <a href=\"https://colab.research.google.com/\">Google Colaboratory</a> has provided powerful TPU kernels for free! For those who cannot afford a powerful GPU can consider shipping your training to Google Colab. Google Colab also provides a familiar Jupyter notebook-like interface, making it very intuitive to use. It is also mounted on your Google Drive, so you can easily read your data into Colab. Weights and logs can also be easily\u00a0saved.</p>\n<p><a href=\"https://medium.com/@renjietan?source=rss-fdf264797c2a------2#7343\"><em>Back to\u00a0overview</em></a></p>\n<h3>Step 5. Google the web and discuss with colleagues to get inspiration</h3>\n<p>With a semi-automated pipeline of fast prototyping and evaluation done in sections 2\u20134, it is time to get inspiration and find creative ways to improve our model\u2019s validation accuracy. Google different search terms, or going to portals like Google Scholar, Science Direct and Pubmed could give us insights. Chatting with colleagues about your problem could give us serendipitous, \u201cEureka\u201d\u00a0moments.</p>\n<p>I was chatting with a colleague who was working on a Natural Language Processing (NLP) project that gave me the inspiration to try a Bi-Directional LSTM (BLSTM) (<a href=\"https://ieeexplore.ieee.org/document/650093\">M. Schuster et al., 1997</a>). The BLSTM reverses the original hidden layers and connects them, allowing a form of generative deep learning, resulting in the output layer getting both information from the past and future states simultaneously. Just by adding a layer of BLSTM, doubled my validation accuracy to\u00a00.465.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/506/1*EagmbkrKX-x98L9Qu7-_zw.png\"><figcaption>Network Diagram of Bi-Directional LSTM\u00a0model</figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/246/1*k33UYW3fqAfmiq5qQZTtlA.png\"><figcaption>Network Diagram of Conv LSTM\u00a0model</figcaption></figure><h4>Conv LSTM\u00a0model</h4>\n<p>The main breakthrough came when I added Convolutional layers for feature extraction. As the input data is a 1D Signal, this model uses a series of 1D Convolutional and 1D Maxpooling layers to extract higher dimensional, latent features before feeding them into 2 LSTM units which capture the temporal information. The output of the LSTM units is then flattened out and we attached a Dropout layer with a dropout rate of 0.5 before adding a Dense layer with a softmax activation to classify all 27\u00a0actions.</p>\n<p>This has got my validation accuracy to 0.700 just on the Inertial data which is the first time we beat the CRC model baseline of 0. For all our models, we used the AdamOptimizer (<a href=\"https://arxiv.org/abs/1412.6980\">D. P. Kingma et al.m 2014</a>) with a<br>learning rate of 1e\u22124, \u03b21 of 0.9, and \u03b22 of 0.999. We initialize our trainable parameters using the Xavier Glorot initializer (<a href=\"http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf\">X. Glorot et at.\u00a0,2010</a>) and set our batch size to 3 to allow our model our model to generalize better (<a href=\"https://arxiv.org/abs/1705.08741\">E. Hoffer et al.,\u00a02017</a>).</p>\n<h4>UNet LSTM\u00a0model</h4>\n<p>The UNet (<a href=\"https://arxiv.org/abs/1505.04597\">O. Ronneberger et al., 2015</a>) is a Fully Convolutional Neural Network (FCNN) that is almost symmetric in the contraction and expansion path. In the contraction path, the input was is being fed through a series of convolutions and max-pooling, increasing the feature maps and decreasing the resolution of the image. This increases the \u201cwhat\u201d and decreases the \u201cwhere\u201d. In the expansion path, the high dimensional features with low resolution are being up-sampled via convolutional kernels. The features maps were reduced during this operation. A novel feature of UNet is that it implements a concatenation of high dimensional features in the contraction path to the low dimensional feature maps of the expansion layers. Similarly, I added the extracted features from the convolutional networks into 2 LSTM units, flattened the output and attached a Dropout layer with a dropout rate of 0.5 finishing off with a Dense layer with a softmax activation to classify all 27 actions. I have attached the Network Diagram in the Appendix\u00a0below.</p>\n<p>The UNet LSTM model achieved a validation accuracy of 0.712 on the Inertial\u00a0data.</p>\n<p><a href=\"https://medium.com/@renjietan?source=rss-fdf264797c2a------2#7343\"><em>Back to\u00a0overview</em></a></p>\n<h3>Step 6. Ensemble your\u00a0models</h3>\n<p>With both Conv LSTM and UNet LSTM performing pretty well on the validation data, we can combine their softmax outputs by taking the average. This immediately increases the validation accuracy to\u00a00.765!</p>\n<p>For most Supervised Learning problems, the ensemble method tends to outperform a single model method. This is currently understood to be because of its ability to transverse the hypothesis space. An ensemble is able to derive a better hypothesis that is not in the hypothesis space of its single models from which it is\u00a0built.</p>\n<p>Empirically, ensembles tend to yield better results when there is diversity among the model (<a href=\"http://machine-learning.martinsewell.com/ensembles/KunchevaWhitaker2003.pdf\">L. Kuncheva et al., 2003</a>). From the Confusion Matrices shown below, we can see that the Conv LSTM is able to pick up actions like swipe to the right and squat better, while the UNet LSTM is able to pick up actions like basketball shoot and draw x better. This indicates that there is model diversity among the two models and true enough, by ensembling them together, we got the validation accuracies from 0.700 and 0.712 to\u00a00.765!</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/766/1*LWo0ebGq4CSs1aX2Aoo4Og.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/767/1*p99IzsqqGfGPtzxWRH2Chw.png\"><figcaption>Confusion Matrices of Conv LSTM (left) and UNet LSTM (right) on Inertial\u00a0data</figcaption></figure><p>Below is the equation I used to create the ensemble. For code implementation, please refer to the\u00a0repo.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/486/1*U36T1fgWG7Y3DbkZegssEw.png\"><figcaption>Average of the softmax output for the Conv LSTM and UNet\u00a0LSTM</figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/366/1*PyoVd_UXzyJB-PIg8XO13g.png\"><figcaption>Softmax output for an action\u00a0j</figcaption></figure><h4>Combining with the Skeleton\u00a0data</h4>\n<p>To achieve the promised 80% validation accuracy as stated in the title, I added the Skeleton data by also resampling it to a period of 180 units. After fusing this with the 6 channel Inertial data, we have an input shape of (N, 180, 66), where N is the number of samples. A table of all the validation accuracies is compiled\u00a0below.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/502/1*4Zhs9aGEm9yYVswxod8TYw.png\"><figcaption>Summary of Validation Accuracy of the Different Models</figcaption></figure><p>Lo and behold, the confusion matrix of our best performing model with a validation accuracy of 0.821 is shown\u00a0below.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/772/1*E1AqJQmUImSrU__g0z3iRw.png\"><figcaption>Confusion Matrix of Ensemble on Inertial + Skeleton\u00a0data</figcaption></figure><p><a href=\"https://medium.com/@renjietan?source=rss-fdf264797c2a------2#7343\"><em>Back to\u00a0overview</em></a></p>\n<h3>Summary</h3>\n<p>Congratulations on making it all the way here! If you have followed these steps thoroughly, you would have successfully built your very own ensembled Human Action Classifier!</p>\n<h4>Model zoo</h4>\n<ul>\n<li><a href=\"https://medium.com/@renjietan?source=rss-fdf264797c2a------2#bd18\">Simple LSTM</a></li>\n<li><a href=\"https://medium.com/@renjietan?source=rss-fdf264797c2a------2#8031\">Bi-Directional LSTM</a></li>\n<li><a href=\"https://medium.com/@renjietan?source=rss-fdf264797c2a------2#1aad\">Conv LSTM</a></li>\n<li><a href=\"https://medium.com/@renjietan?source=rss-fdf264797c2a------2#5fc1\">UNet LSTM</a></li>\n</ul>\n<h4>Some key takeaways</h4>\n<ul>\n<li>Plotting is a quick and easy way to understand your\u00a0data</li>\n<li>Data Science is a highly iterative process</li>\n<li>Automate the things you\u00a0can</li>\n<li>Ensemble is a quick way to get the best bang for your buck of our trained\u00a0models</li>\n<li>Use Google Colab to increase your training\u00a0speed</li>\n<li>Keras is the framework of choice for quick prototyping of deep learning\u00a0models</li>\n</ul>\n<p>If you are up for a challenge and feel that 0.821 is not enough, you may read the following subsection to improve your\u00a0model.</p>\n<h4>What more could be\u00a0done</h4>\n<h4>A. Issue of over-fitting</h4>\n<p>Throughout our training, over-fitting at early epochs seems to be the main recurring challenge that we faced. We tried adding Dropout layers and ensembling to make our model more generalized but we can still go further. Over-fitting tends to happen when our model tries to learn high-frequency features that may not be useful. Adding Gaussian Noise with zero mean and data points in all frequencies might enhance the learning capability of our<br>model. Similarly, the time sequences of different subjects are quite varied even for the same activities. Performing data augmentation using time scaling and translation would increase the amount of training data, allowing our model to generalize better.</p>\n<p>On a side note, our model could also be trimmed further to reduce its complexity, and also its risk of over-fitting. With the recent Neural Architecture Search papers like NAS (<a href=\"https://arxiv.org/abs/1611.01578\">B. Zoph et.al, 2016</a>), NASnet (<a href=\"https://arxiv.org/abs/1707.07012\">B.Zpoh et.al, 2017</a>) and Efficient-NAS (<a href=\"https://arxiv.org/abs/1802.03268\">H. Pham et.al, 2018</a>), gaining traction, we could also try applying them since this is also a classification task.</p>\n<h4>B. Data Fusion of RGB and Depth\u00a0Data</h4>\n<p>We played with the Inertial, and we added the Skeleton towards the end to get us more information to find our data-hungry models. In order to push our model more, we would have to find ways to fusion it with the Depth and RGB data. This would allow for more input training variables to learn and extract features from, hence improving the validation accuracies.</p>\n<h4>C. Try other Ensemble Learning Techniques</h4>\n<p>Instead of doing a simple average, we could try more advanced ensemble learning approaches such as Boosting and\u00a0Bagging.</p>\n<p><em>Special thanks to </em><a href=\"https://towardsdatascience.com/@raimibinkarim\"><em>Raimi</em></a><em> and </em><a href=\"https://medium.com/@derekchia\"><em>Derek</em></a><em> for proofreading and giving me feedback on this\u00a0article.</em></p>\n<p><em>Feel free to connect with me via </em><a href=\"https://twitter.com/rahjaytee\"><em>Twitter</em></a><em>, </em><a href=\"https://www.linkedin.com/in/renjietan/\"><em>LinkedIn</em></a><em>!</em></p>\n<p><em>If you are interested in other projects that I have worked on, feel free to visit my\u00a0</em><a href=\"https://github.com/notha99y\"><em>Github</em></a><em>!</em></p>\n<h3>Appendix</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/460/1*pR8nx6BDrc7vavUzcY1Wfw.png\"><figcaption>Network Diagram of UNet LSTM\u00a0model</figcaption></figure><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=655fcb8781c5\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://towardsdatascience.com/6-steps-to-quickly-train-a-human-action-classifier-with-validation-accuracy-of-over-80-655fcb8781c5\">6 Steps to quickly train a Human Action Classifier with Validation Accuracy of over 80%</a> was originally published in <a href=\"https://towardsdatascience.com/\">Towards Data Science</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","enclosure":{},"categories":["recurrent-neural-network","machine-learning","keras","convolutional-network","data-science"]}]}